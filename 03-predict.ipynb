{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Collecting git+https://github.com/rbilleci/pandora.git\n",
      "  Cloning https://github.com/rbilleci/pandora.git to /tmp/pip-req-build-rsfc7_wp\n",
      "  Running command git clone -q https://github.com/rbilleci/pandora.git /tmp/pip-req-build-rsfc7_wp\n",
      "Requirement already satisfied (use --upgrade to upgrade): pandora==0.1.0 from git+https://github.com/rbilleci/pandora.git in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: pandas~=1.2.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: fnvhash~=0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn~=0.24.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.24.1)\n",
      "Requirement already satisfied: workalendar~=14.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: category-encoders~=2.2.2 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: pyCalverter in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: skyfield in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.36)\n",
      "Requirement already satisfied: lunardate in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: skyfield-data in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pyluach in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: setuptools>=1.0 in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas~=1.2.1->pandora==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2019.11.28)\n",
      "Requirement already satisfied: sgp4>=2.2 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Requirement already satisfied: jplephem>=2.13 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Building wheels for collected packages: pandora\n",
      "  Building wheel for pandora (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandora: filename=pandora-0.1.0-py3-none-any.whl size=2681412 sha256=120d54e423f81eed34930b641a9b812ab006267ba82c501f53b465521b01f4b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b8nfv62n/wheels/01/8b/d5/a72c927a738750e04a4bb4fd22f63b4b88c7b5871732e2d67b\n",
      "Successfully built pandora\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install git+https://github.com/rbilleci/pandora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from pandora import loader, encoders\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (92) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and set the date column\n",
    "df = pd.read_csv('temp/01-data.csv', keep_default_na=False, na_values='')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['region_name'] = df['region_name'].fillna('')\n",
    "\n",
    "# determine the date where prediction should begin\n",
    "prediction_start_date = df[df['predicted'] == True]['date'].min().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# declare the encoders\n",
    "enc = {}\n",
    "enc['continent'] = encoders.OneHotEncoder('continent')\n",
    "enc['geo_code'] = encoders.BinaryEncoder('geo_code')\n",
    "enc['country_code'] = encoders.BinaryEncoder('country_code')\n",
    "enc['day_of_week'] = encoders.OneHotEncoder('day_of_week')\n",
    "enc['day_of_month'] = encoders.BinaryEncoder('day_of_month')\n",
    "enc['day_of_month_cyc'] = encoders.CyclicalEncoder('day_of_month')\n",
    "enc['day_of_year'] = encoders.BinaryEncoder('day_of_year')\n",
    "enc['day_of_year_cyc'] = encoders.CyclicalEncoder('day_of_year')\n",
    "\n",
    "def encode(df_x, fit):\n",
    "    \n",
    "    # encode the geo data\n",
    "    if fit:\n",
    "        df_x = enc['continent'].fit_transform(df_x)\n",
    "        df_x = enc['geo_code'].fit_transform(df_x)\n",
    "        df_x = enc['country_code'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['continent'].transform(df_x)\n",
    "        df_x = enc['geo_code'].transform(df_x)\n",
    "        df_x = enc['country_code'].transform(df_x)\n",
    "        \n",
    "    if fit:\n",
    "        df_x = enc['day_of_week'].fit_transform(df_x)\n",
    "        #df_x['day_of_week'] = df_x['day_of_week'].apply(lambda x: x / 7.)\n",
    "        #df_x = enc['day_of_week_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['day_of_week'].transform(df_x)\n",
    "        #df_x['day_of_week'] = df_x['day_of_week'].apply(lambda x: x / 7.)\n",
    "        #df_x = enc['day_of_week_cyc'].transform(df_x)\n",
    "        \n",
    "    if fit:\n",
    "        df_x['day_of_month'] = df_x['day_of_month'].apply(lambda x: x / 31.)\n",
    "        df_x = enc['day_of_month_cyc'].fit_transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'].apply(lambda x: x / 366.)\n",
    "        df_x = enc['day_of_year_cyc'].fit_transform(df_x)\n",
    "    else:    \n",
    "        df_x['day_of_month'] = df_x['day_of_month'].apply(lambda x: x / 31.)\n",
    "        df_x = enc['day_of_month_cyc'].transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'].apply(lambda x: x / 366.)\n",
    "        df_x = enc['day_of_year_cyc'].transform(df_x)\n",
    "    \n",
    "    # convert the date to an integer value\n",
    "    df_x['date_day'] = df_x['date'].apply(lambda x: x.day)\n",
    "    \n",
    "    # drop unused columns\n",
    "    for column in df_x.columns:\n",
    "        if column.find('--') >= 0:\n",
    "            df_x = df_x.drop(labels=[column], axis=1)\n",
    "\n",
    "    #for column in df_x.columns:\n",
    "    #    if column.find('_21') >= 0:\n",
    "    #        df_x = df_x.drop(labels=[column], axis=1)\n",
    "\n",
    "    #for column in df_x.columns:\n",
    "    #    if column.find('age_dist') >= 0:\n",
    "    #        df_x = df_x.drop(labels=[column], axis=1)\n",
    "            \n",
    "    df_x = df_x.drop(labels=['country_name',\n",
    "                           'continent',\n",
    "                           'country_code',\n",
    "                           'day_of_week',\n",
    "                           'day_of_month',\n",
    "                           'day_of_year',\n",
    "                            'npi_sum',\n",
    "                            'pneumonia_deaths_per_100k',\n",
    "                            'country_code3',\n",
    "                            'country_code_numeric',\n",
    "                            'confirmed_deaths',\n",
    "                            'predicted',\n",
    "                            'region_name',\n",
    "                            'month',                           \n",
    "                            'quarter',\n",
    "                            'week',\n",
    "                            'temperature',\n",
    "                            'year'], axis=1)\n",
    "    return df_x\n",
    "\n",
    "# only work within the specified range\n",
    "df_ml = df.loc[df['date'] < pd.to_datetime(prediction_start_date)]\n",
    "df_ml = encode(df_ml, fit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train, val, test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Range:   2020-01-01 - 2021-01-27\n",
      "Validation Range: 2021-01-28 - 2021-02-03\n",
      "Test Range:       2021-02-04 - 2021-02-10\n"
     ]
    }
   ],
   "source": [
    "days_for_validation = 7\n",
    "days_for_test = 7\n",
    "\n",
    "def split(df: pd.DataFrame, \n",
    "          days_for_validation: int, \n",
    "          days_for_test: int) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # First, sort the data by date\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Determine the maximum date\n",
    "    date_start_test = df['date'].max() - pd.to_timedelta(days_for_test - 1, unit='d')\n",
    "    date_start_validation = date_start_test - pd.to_timedelta(days_for_validation, unit='d')\n",
    "\n",
    "    df_train = df[df['date'] < date_start_validation]\n",
    "    df_validation = df[(df['date'] >= date_start_validation) & (df['date'] < date_start_test)]\n",
    "    df_test = df[df['date'] >= date_start_test]\n",
    "\n",
    "    # Debug the outpoint\n",
    "    print(f\"Training Range:   {df_train['date'].min().date()} - {df_train['date'].max().date()}\")\n",
    "    print(f\"Validation Range: {df_validation['date'].min().date()} - {df_validation['date'].max().date()}\")\n",
    "    print(f\"Test Range:       {df_test['date'].min().date()} - {df_test['date'].max().date()}\")\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(df.index) != len(df_train.index) + len(df_validation.index) + len(df_test.index):\n",
    "        raise Exception('entries do not add up')\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "df_train_prescaled, df_validation_prescaled, df_test_prescaled = split(df_ml, days_for_validation, days_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_prescaled.copy()\n",
    "df_validation = df_validation_prescaled.copy()\n",
    "df_test = df_test_prescaled.copy()\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "for feature_name in df_ml.columns.values:\n",
    "    if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "        continue\n",
    "    scalers[feature_name] = RobustScaler()\n",
    "    df_train[feature_name] = scalers[feature_name].fit_transform(df_train_prescaled[[feature_name]])\n",
    "        \n",
    "if len(df_validation_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue\n",
    "        df_validation[feature_name] = scalers[feature_name].transform(df_validation_prescaled[[feature_name]])\n",
    "\n",
    "if len(df_test_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue        \n",
    "        df_test[feature_name] = scalers[feature_name].transform(df_test_prescaled[[feature_name]])\n",
    "\n",
    "df_train = df_train.drop(labels=['geo_code', 'date'], axis=1)\n",
    "df_validation = df_validation.drop(labels=['geo_code', 'date'], axis=1)\n",
    "df_test = df_test.drop(labels=['geo_code', 'date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:9953.99219\ttrain-rmse:4404.41455\n",
      "[10]\teval-rmse:4015.84521\ttrain-rmse:3412.61694\n",
      "[20]\teval-rmse:3424.13452\ttrain-rmse:3343.23999\n",
      "[30]\teval-rmse:3346.97681\ttrain-rmse:3318.09058\n",
      "[40]\teval-rmse:3319.98535\ttrain-rmse:3307.35010\n",
      "[50]\teval-rmse:3299.04126\ttrain-rmse:3301.81348\n",
      "[60]\teval-rmse:3279.66138\ttrain-rmse:3298.21045\n",
      "[70]\teval-rmse:3261.22607\ttrain-rmse:3295.37500\n",
      "[80]\teval-rmse:3243.66284\ttrain-rmse:3292.92139\n",
      "[90]\teval-rmse:3227.05713\ttrain-rmse:3290.72925\n",
      "[100]\teval-rmse:3211.46045\ttrain-rmse:3288.74341\n",
      "[110]\teval-rmse:3196.89062\ttrain-rmse:3286.93506\n",
      "[120]\teval-rmse:3183.32812\ttrain-rmse:3285.30322\n",
      "[130]\teval-rmse:3170.72583\ttrain-rmse:3283.82739\n",
      "[140]\teval-rmse:3159.02319\ttrain-rmse:3282.49976\n",
      "[150]\teval-rmse:3148.15698\ttrain-rmse:3281.30640\n",
      "[160]\teval-rmse:3138.07275\ttrain-rmse:3280.22852\n",
      "[170]\teval-rmse:3128.69971\ttrain-rmse:3279.26269\n",
      "[180]\teval-rmse:3119.99145\ttrain-rmse:3278.39600\n",
      "[190]\teval-rmse:3111.89209\ttrain-rmse:3277.61914\n",
      "[200]\teval-rmse:3104.35303\ttrain-rmse:3276.92481\n",
      "[210]\teval-rmse:3097.33252\ttrain-rmse:3276.29956\n",
      "[220]\teval-rmse:3090.79541\ttrain-rmse:3275.74561\n",
      "[230]\teval-rmse:3084.69556\ttrain-rmse:3275.24731\n",
      "[240]\teval-rmse:3079.01001\ttrain-rmse:3274.80493\n",
      "[250]\teval-rmse:3073.70215\ttrain-rmse:3274.41016\n",
      "[260]\teval-rmse:3068.74707\ttrain-rmse:3274.05566\n",
      "[270]\teval-rmse:3064.12183\ttrain-rmse:3273.73706\n",
      "[280]\teval-rmse:3059.79517\ttrain-rmse:3273.45557\n",
      "[290]\teval-rmse:3055.75415\ttrain-rmse:3273.20630\n",
      "[300]\teval-rmse:3051.97314\ttrain-rmse:3272.98022\n",
      "[310]\teval-rmse:3048.43555\ttrain-rmse:3272.78369\n",
      "[320]\teval-rmse:3045.12500\ttrain-rmse:3272.60474\n",
      "[330]\teval-rmse:3042.02563\ttrain-rmse:3272.44629\n",
      "[340]\teval-rmse:3039.12646\ttrain-rmse:3272.30664\n",
      "[350]\teval-rmse:3036.40894\ttrain-rmse:3272.18335\n",
      "[360]\teval-rmse:3033.86206\ttrain-rmse:3272.06982\n",
      "[370]\teval-rmse:3031.47437\ttrain-rmse:3271.97168\n",
      "[380]\teval-rmse:3029.23755\ttrain-rmse:3271.88452\n",
      "[390]\teval-rmse:3027.14209\ttrain-rmse:3271.80737\n",
      "[400]\teval-rmse:3025.17505\ttrain-rmse:3271.73828\n",
      "[410]\teval-rmse:3023.33032\ttrain-rmse:3271.67529\n",
      "[420]\teval-rmse:3021.59888\ttrain-rmse:3271.61963\n",
      "[430]\teval-rmse:3019.97412\ttrain-rmse:3271.57300\n",
      "[440]\teval-rmse:3018.45215\ttrain-rmse:3271.53003\n",
      "[450]\teval-rmse:3017.01856\ttrain-rmse:3271.48926\n",
      "[460]\teval-rmse:3015.67651\ttrain-rmse:3271.44873\n",
      "[470]\teval-rmse:3014.41480\ttrain-rmse:3271.41846\n",
      "[480]\teval-rmse:3013.23144\ttrain-rmse:3271.39356\n",
      "[490]\teval-rmse:3012.11914\ttrain-rmse:3271.36743\n",
      "[499]\teval-rmse:3011.17725\ttrain-rmse:3271.34741\n",
      "5396.085933993931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "params_tree = {\n",
    "    'nthread': 1,\n",
    "    'objective':'reg:squarederror',\n",
    "}\n",
    "\n",
    "\n",
    "params_linear = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "params_dart = {\n",
    "    \"booster\": \"dart\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "\n",
    "rounds = 500\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "test_x, test_y = df_test.iloc[:, 1:], df_test.iloc[:, :1]\n",
    "dtest = xgb.DMatrix(data=test_x,label=test_y)\n",
    "callback_monitor = xgb.callback.EvaluationMonitor(rank=0, period=10, show_stdv=False)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=df_train.iloc[:, 1:], label=df_train.iloc[:, :1])\n",
    "dvalidation = xgb.DMatrix(data=df_validation.iloc[:, 1:], label=df_validation.iloc[:, :1])\n",
    "watchlist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(params_linear, \n",
    "                dtrain, \n",
    "                rounds,\n",
    "                watchlist,\n",
    "                early_stopping_rounds=20,      \n",
    "                callbacks=[callback_monitor],\n",
    "                verbose_eval=False)\n",
    "bst.save_model('temp/predictor.model')\n",
    "predictions = bst.predict(dtest)\n",
    "score = mean_squared_error(test_y, predictions, squared=False)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the Prescription Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-13 01:56:56.559975 - generating predictions for index 0\n",
      "2021-02-13 01:56:56.573800 - applying intervention plan\n",
      "2021-02-13 01:57:08.211729 - predicting\n",
      "2021-02-13 01:59:10.925045 - generating predictions for index 1\n",
      "2021-02-13 01:59:10.928882 - applying intervention plan\n",
      "2021-02-13 01:59:22.009899 - predicting\n",
      "2021-02-13 02:01:25.859971 - generating predictions for index 2\n",
      "2021-02-13 02:01:25.863200 - applying intervention plan\n",
      "2021-02-13 02:01:36.936465 - predicting\n",
      "2021-02-13 02:03:39.871810 - generating predictions for index 3\n",
      "2021-02-13 02:03:39.874964 - applying intervention plan\n",
      "2021-02-13 02:03:51.004173 - predicting\n",
      "2021-02-13 02:05:53.824849 - generating predictions for index 4\n",
      "2021-02-13 02:05:53.828093 - applying intervention plan\n",
      "2021-02-13 02:06:04.904419 - predicting\n",
      "2021-02-13 02:08:08.200647 - generating predictions for index 5\n",
      "2021-02-13 02:08:08.203854 - applying intervention plan\n",
      "2021-02-13 02:08:19.410487 - predicting\n",
      "2021-02-13 02:10:22.774047 - generating predictions for index 6\n",
      "2021-02-13 02:10:22.777443 - applying intervention plan\n",
      "2021-02-13 02:10:34.179492 - predicting\n",
      "2021-02-13 02:12:37.705902 - generating predictions for index 7\n",
      "2021-02-13 02:12:37.709089 - applying intervention plan\n",
      "2021-02-13 02:12:48.814947 - predicting\n",
      "2021-02-13 02:14:51.452874 - generating predictions for index 8\n",
      "2021-02-13 02:14:51.456279 - applying intervention plan\n",
      "2021-02-13 02:15:02.623835 - predicting\n",
      "2021-02-13 02:17:05.341031 - generating predictions for index 9\n",
      "2021-02-13 02:17:05.344275 - applying intervention plan\n",
      "2021-02-13 02:17:16.503137 - predicting\n",
      "2021-02-13 02:19:19.259905 - evaluating baselin plan\n",
      "2021-02-13 02:19:19.411448 - predicting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_geo_date_index(df_x):\n",
    "    df_x['_index'] = df_x['CountryName'] + df_x['RegionName'] + df_x['Date'].dt.strftime('%Y%m%d')\n",
    "    return df_x\n",
    "    \n",
    "\n",
    "def map_ips(row, \n",
    "            df_intervention_plan,\n",
    "            date_to_predict_from,\n",
    "            date_to_predict_to):\n",
    "    date = row['date']\n",
    "    if date < date_to_predict_from:\n",
    "        return row\n",
    "    if date > date_to_predict_to:\n",
    "        return row\n",
    "    country_name = row['country_name']\n",
    "    region_name = row['region_name']\n",
    "    key = country_name + region_name + date.strftime('%Y%m%d')\n",
    "    ip_row = df_intervention_plan.loc[df_intervention_plan['_index'] == key]\n",
    "    row['c1_school_closing'] = ip_row['C1_School closing'].max()\n",
    "    row['c2_workplace_closing'] = ip_row['C2_Workplace closing'].max()\n",
    "    row['c3_cancel_public_events'] = ip_row['C3_Cancel public events'].max()\n",
    "    row['c4_restrictions_on_gatherings'] = ip_row['C4_Restrictions on gatherings'].max()\n",
    "    row['c5_close_public_transport'] = ip_row['C5_Close public transport'].max()\n",
    "    row['c6_stay_at_home_requirements'] = ip_row['C6_Stay at home requirements'].max()\n",
    "    row['c7_restrictions_on_internal_movement'] = ip_row['C7_Restrictions on internal movement'].max()\n",
    "    row['c8_international_travel_controls'] = ip_row['C8_International travel controls'].max()\n",
    "    row['h1_public_information_campaigns'] = ip_row['H1_Public information campaigns'].max()\n",
    "    row['h2_testing_policy'] = ip_row['H2_Testing policy'].max()\n",
    "    row['h3_contact_tracing'] = ip_row['H3_Contact tracing'].max()\n",
    "    row['h6_facial_coverings'] = ip_row['H6_Facial Coverings'].max()    \n",
    "    return row\n",
    "\n",
    "# read the prescriptions and generate a prediction file for each prescription\n",
    "def evaluate_intervention_plans(days):\n",
    "    df_intervention_plans = pd.read_csv('prescriptions.csv', keep_default_na=False, na_values='')\n",
    "    df_intervention_plans['RegionName'] = df_intervention_plans['RegionName'].fillna('')\n",
    "    df_intervention_plans['Date'] = pd.to_datetime(df_intervention_plans['Date']) \n",
    "    df_intervention_plans = add_geo_date_index(df_intervention_plans)\n",
    "    for i in range(10):\n",
    "        print(f\"{datetime.datetime.now()} - generating predictions for index {i}\")\n",
    "        df_intervention_plan = df_intervention_plans.loc[df_intervention_plans['PrescriptionIndex'] == i]\n",
    "        evaluate_intervention_plan(i, df_intervention_plan, days)\n",
    "\n",
    "\n",
    "def evaluate_intervention_plan(prescription_index, \n",
    "                               df_intervention_plan, \n",
    "                               days_to_predict):\n",
    "    print(f\"{datetime.datetime.now()} - applying intervention plan\")\n",
    "    date_to_predict_from = df[df['predicted'] == False]['date'].max()\n",
    "    date_to_predict_to = date_to_predict_from + pd.to_timedelta(days_to_predict, unit='d')  \n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.loc[df_out['date'] <= pd.to_datetime(date_to_predict_to) + pd.to_timedelta(1, unit='d')]\n",
    "    df_out = df_out.sort_values(['geo_code', 'date'])\n",
    "    \n",
    "    df_out = df_out.apply(lambda row: map_ips(row, \n",
    "                                              df_intervention_plan, \n",
    "                                              date_to_predict_from, \n",
    "                                              date_to_predict_to), axis=1)\n",
    "    \n",
    "    prediction = predict(df_out, date_to_predict_from, date_to_predict_to)\n",
    "    prediction.to_csv(f\"_plan_{prescription_index}.csv\", index=False)\n",
    "\n",
    "def predict_baseline(days_to_predict):\n",
    "    print(f\"{datetime.datetime.now()} - evaluating baselin plan\")\n",
    "    date_to_predict_from = df[df['predicted'] == False]['date'].max()\n",
    "    date_to_predict_to = date_to_predict_from + pd.to_timedelta(days_to_predict, unit='d')  \n",
    "    df_out = df.copy()\n",
    "    df_out = df_out[df_out['date'] <= pd.to_datetime(date_to_predict_to) + pd.to_timedelta(1, unit='d')  ]\n",
    "    df_out = df_out.sort_values(['geo_code', 'date'])        \n",
    "    prediction = predict(df_out, date_to_predict_from, date_to_predict_to)    \n",
    "    prediction.to_csv(f\"_plan_baseline.csv\", index=False)\n",
    "    \n",
    "def predict(df_out, \n",
    "            date_to_predict_from, \n",
    "            date_to_predict_to):\n",
    "    # predict each country\n",
    "    print(f\"{datetime.datetime.now()} - predicting\")\n",
    "    df_out = df_out.groupby(['geo_code']).apply(\n",
    "        lambda g: predict_for_geo(g, date_to_predict_from, date_to_predict_to)).reset_index(0, drop=True)\n",
    "    \n",
    "    # filter out any extra days\n",
    "    df_out = df_out[df_out['date'] <= pd.to_datetime(date_to_predict_to)]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def predict_for_geo(df_geo,\n",
    "                    date_to_predict_from, \n",
    "                    date_to_predict_to):\n",
    "    geo_code = df_geo['geo_code'].iloc[0]\n",
    "    df_input = df_geo.copy()\n",
    "    df_input = encode(df_input, fit=False)\n",
    "\n",
    "    # scale values\n",
    "    for name in scalers:\n",
    "        if name not in df_input.columns:\n",
    "            continue\n",
    "        if name == 'confirmed_cases':\n",
    "            continue\n",
    "        if name == 'new_cases':\n",
    "            continue\n",
    "        df_input[name] = scalers[name].transform(df_input[[name]])\n",
    "    \n",
    "    # get iterators we'll use on the rows\n",
    "    df_geo_it = df_geo.loc[df_geo['date'] >= date_to_predict_from].iterrows()\n",
    "    df_input_it = df_input.loc[df_input['date'] >= date_to_predict_from].iterrows()\n",
    "    \n",
    "    # predict each day\n",
    "    new_cases = 0.\n",
    "    new_cases_ma = []\n",
    "    confirmed_cases = 0.\n",
    "    confirmed_cases_ma = []\n",
    "    first_row = True\n",
    "    \n",
    "    for x, row in df_geo_it:\n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            new_cases = row['new_cases']\n",
    "            new_cases_ma = [new_cases] * 21\n",
    "            confirmed_cases = row['confirmed_cases']     \n",
    "            confirmed_cases_ma = [confirmed_cases] * 21       \n",
    "        else:            \n",
    "            row['new_cases'] = new_cases\n",
    "            row['new_cases_as_percent_of_population'] = new_cases / row['population']              \n",
    "            row['confirmed_cases'] = confirmed_cases\n",
    "            row['confirmed_cases_as_percent_of_population'] = confirmed_cases / row['population']        \n",
    "        \n",
    "        # update the moving averages for this row\n",
    "        for window_size in [3, 7, 21]:\n",
    "            row[f\"new_cases_ma_{window_size}\"] = np.mean(new_cases_ma[-window_size:])\n",
    "            row[f\"confirmed_cases_ma_{window_size}\"] = np.mean(confirmed_cases_ma[-window_size:])\n",
    "        \n",
    "        # get the dates\n",
    "        date_from = row['date']\n",
    "        date_to = date_from + pd.to_timedelta(1, unit='d')  \n",
    "        \n",
    "        # make sure we are not predicting too far\n",
    "        if date_from > date_to_predict_to:\n",
    "            break\n",
    "        \n",
    "        # convert to df\n",
    "        model_input = next(df_input_it)[1]    \n",
    "        model_input = pd.DataFrame(model_input.to_frame().T)\n",
    "        model_input['new_cases'] = new_cases\n",
    "        model_input['new_cases_as_percent_of_population'] = row['new_cases_as_percent_of_population']\n",
    "        model_input['confirmed_cases'] = confirmed_cases\n",
    "        model_input['confirmed_cases_as_percent_of_population'] = row['confirmed_cases_as_percent_of_population']\n",
    "        model_input = model_input.drop(labels=['geo_code', 'date'], axis=1)\n",
    "        for name in model_input.columns:\n",
    "            model_input[name] = pd.to_numeric(model_input[name])\n",
    "        model_input['new_cases'] = scalers['new_cases'].transform(model_input[['new_cases']])        \n",
    "        model_input['new_cases_as_percent_of_population'] = scalers['new_cases_as_percent_of_population'].transform(model_input[['new_cases_as_percent_of_population']])        \n",
    "        model_input['confirmed_cases'] = scalers['confirmed_cases'].transform(model_input[['confirmed_cases']])\n",
    "        model_input['confirmed_cases_as_percent_of_population'] = scalers['confirmed_cases_as_percent_of_population'].transform(model_input[['confirmed_cases_as_percent_of_population']])\n",
    "\n",
    "        # predict\n",
    "        predictions = bst.predict(xgb.DMatrix(data=model_input.iloc[:, 1:], label=model_input.iloc[:, :1]))\n",
    "\n",
    "        # update calculations\n",
    "        new_cases = max(0., predictions[0])\n",
    "        confirmed_cases += new_cases\n",
    "        # add to moving averages\n",
    "        new_cases_ma.append(new_cases)\n",
    "        confirmed_cases_ma.append(confirmed_cases) \n",
    "        # assign the row\n",
    "        df_geo.loc[x] = row\n",
    "        \n",
    "    return df_geo\n",
    " \n",
    "\n",
    "evaluate_intervention_plans(3)\n",
    "predict_baseline(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-a1823d221b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geo_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_cases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confirmed_cases'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_predict' is not defined"
     ]
    }
   ],
   "source": [
    "df_predict.loc[df_predict['geo_code'] == 'DE'][['date', 'new_cases', 'confirmed_cases']].tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
