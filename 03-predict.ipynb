{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Collecting git+https://github.com/rbilleci/pandora.git\n",
      "  Cloning https://github.com/rbilleci/pandora.git to /tmp/pip-req-build-a7w_ctbc\n",
      "  Running command git clone -q https://github.com/rbilleci/pandora.git /tmp/pip-req-build-a7w_ctbc\n",
      "Requirement already satisfied (use --upgrade to upgrade): pandora==0.1.0 from git+https://github.com/rbilleci/pandora.git in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: pandas~=1.2.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: fnvhash~=0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn~=0.24.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.24.1)\n",
      "Requirement already satisfied: workalendar~=14.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: category-encoders~=2.2.2 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: skyfield-data in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pyluach in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: skyfield in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.36)\n",
      "Requirement already satisfied: lunardate in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: pyCalverter in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: setuptools>=1.0 in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas~=1.2.1->pandora==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: jplephem>=2.13 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2019.11.28)\n",
      "Requirement already satisfied: sgp4>=2.2 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Building wheels for collected packages: pandora\n",
      "  Building wheel for pandora (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandora: filename=pandora-0.1.0-py3-none-any.whl size=2681412 sha256=c3747069c2561901e403d263153eb5ed96c5ab86d6f89b30d892701e81850a19\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2gablfqc/wheels/01/8b/d5/a72c927a738750e04a4bb4fd22f63b4b88c7b5871732e2d67b\n",
      "Successfully built pandora\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install git+https://github.com/rbilleci/pandora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from pandora import loader, encoders\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (98) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and set the date column\n",
    "df = pd.read_csv('temp/01-data.csv', keep_default_na=False, na_values='')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# determine the date where prediction should begin\n",
    "prediction_start_date = df[df['predicted'] == True]['date'].min().date()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# declare the encoders\n",
    "enc = {}\n",
    "enc['continent'] = encoders.BinaryEncoder('continent')\n",
    "enc['geo_code'] = encoders.BinaryEncoder('geo_code')\n",
    "enc['country_code'] = encoders.BinaryEncoder('country_code')\n",
    "enc['day_of_week'] = encoders.BinaryEncoder('day_of_week')\n",
    "enc['day_of_week_cyc'] = encoders.CyclicalEncoder('day_of_week')\n",
    "enc['day_of_month'] = encoders.BinaryEncoder('day_of_month')\n",
    "enc['day_of_month_cyc'] = encoders.CyclicalEncoder('day_of_month')\n",
    "enc['day_of_year'] = encoders.BinaryEncoder('day_of_year')\n",
    "enc['day_of_year_cyc'] = encoders.CyclicalEncoder('day_of_year')\n",
    "\n",
    "def encode(df_x, fit):\n",
    "    # convert the date to an integer value\n",
    "    df_x['date_day'] = df_x['date'].apply(lambda x: x.day)\n",
    "\n",
    "    # encode the geo data\n",
    "    if fit:\n",
    "        df_x = enc['continent'].fit_transform(df_x)\n",
    "        df_x = enc['geo_code'].fit_transform(df_x)\n",
    "        df_x = enc['country_code'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['continent'].transform(df_x)\n",
    "        df_x = enc['geo_code'].transform(df_x)\n",
    "        df_x = enc['country_code'].transform(df_x)\n",
    "    if fit:\n",
    "        df_x = enc['day_of_week'].fit_transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'] / 7.0\n",
    "        df_x = enc['day_of_week_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['day_of_week'].transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'] / 7.0\n",
    "        df_x = enc['day_of_week_cyc'].transform(df_x)\n",
    "    if fit:\n",
    "        df_x['day_of_month'] = df_x['day_of_month'] / 31.0 # keep it simple\n",
    "        df_x = enc['day_of_month_cyc'].fit_transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'] / 366.0 # keep it simple\n",
    "        df_x = enc['day_of_year_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x['day_of_month'] = df_x['day_of_month'] / 31.0 # keep it simple\n",
    "        df_x = enc['day_of_month_cyc'].transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'] / 366.0 # keep it simple\n",
    "        df_x = enc['day_of_year_cyc'].transform(df_x)\n",
    "        \n",
    "    \n",
    "    # drop unused columns\n",
    "    df_x = df_x.drop(labels=['country_name',\n",
    "                           'continent',\n",
    "                           'geo_code',\n",
    "                           'country_code',\n",
    "                           'day_of_week',\n",
    "                           'day_of_month',\n",
    "                           'day_of_year',\n",
    "                            'country_code3',\n",
    "                            'country_code_numeric',\n",
    "                            'confirmed_deaths',\n",
    "                            'predicted',\n",
    "                            'region_name',\n",
    "                            'month',                           \n",
    "                            'quarter',\n",
    "                            'week'], axis=1)\n",
    "    return df_x\n",
    "\n",
    "# only work within the specified range\n",
    "df_ml = df.loc[df['date'] < pd.to_datetime(prediction_start_date)]\n",
    "df_ml = encode(df_ml, fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train, val, test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Range:   2020-01-01 - 2020-12-25\n",
      "Validation Range: 2020-12-26 - 2021-01-25\n",
      "Test Range:       2021-01-26 - 2021-02-08\n"
     ]
    }
   ],
   "source": [
    "days_for_validation = 31\n",
    "days_for_test = 14\n",
    "\n",
    "def split(df: pd.DataFrame, \n",
    "          days_for_validation: int, \n",
    "          days_for_test: int) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # First, sort the data by date\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Determine the maximum date\n",
    "    date_start_test = df['date'].max() - pd.to_timedelta(days_for_test - 1, unit='d')\n",
    "    date_start_validation = date_start_test - pd.to_timedelta(days_for_validation, unit='d')\n",
    "\n",
    "    df_train = df[df['date'] < date_start_validation]\n",
    "    df_validation = df[(df['date'] >= date_start_validation) & (df['date'] < date_start_test)]\n",
    "    df_test = df[df['date'] >= date_start_test]\n",
    "\n",
    "    # Debug the outpoint\n",
    "    print(f\"Training Range:   {df_train['date'].min().date()} - {df_train['date'].max().date()}\")\n",
    "    print(f\"Validation Range: {df_validation['date'].min().date()} - {df_validation['date'].max().date()}\")\n",
    "    print(f\"Test Range:       {df_test['date'].min().date()} - {df_test['date'].max().date()}\")\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(df.index) != len(df_train.index) + len(df_validation.index) + len(df_test.index):\n",
    "        raise Exception('entries do not add up')\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "df_train_prescaled, df_validation_prescaled, df_test_prescaled = split(df_ml, days_for_validation, days_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_prescaled.copy()\n",
    "df_validation = df_validation_prescaled.copy()\n",
    "df_test = df_test_prescaled.copy()\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "for feature_name in df_ml.columns.values:\n",
    "    if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "        continue\n",
    "    scalers[feature_name] = StandardScaler()\n",
    "    df_train[feature_name] = scalers[feature_name].fit_transform(df_train_prescaled[[feature_name]])\n",
    "        \n",
    "if len(df_validation_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue\n",
    "        df_validation[feature_name] = scalers[feature_name].transform(df_validation_prescaled[[feature_name]])\n",
    "\n",
    "if len(df_test_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue        \n",
    "        df_test[feature_name] = scalers[feature_name].transform(df_test_prescaled[[feature_name]])\n",
    "\n",
    "df_train = df_train.drop(labels=['date'], axis=1)\n",
    "df_validation = df_validation.drop(labels=['date'], axis=1)\n",
    "df_test = df_test.drop(labels=['date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:8091.68066\ttrain-rmse:4131.48633\n",
      "[10]\teval-rmse:4195.98486\ttrain-rmse:3324.66919\n",
      "[20]\teval-rmse:4029.15161\ttrain-rmse:3270.61499\n",
      "[30]\teval-rmse:4028.06714\ttrain-rmse:3254.31982\n",
      "[40]\teval-rmse:4027.02539\ttrain-rmse:3247.43579\n",
      "[50]\teval-rmse:4021.57471\ttrain-rmse:3243.72827\n",
      "[60]\teval-rmse:4013.83936\ttrain-rmse:3241.34839\n",
      "[70]\teval-rmse:4005.06616\ttrain-rmse:3239.60889\n",
      "[80]\teval-rmse:3995.98877\ttrain-rmse:3238.23267\n",
      "[90]\teval-rmse:3987.03931\ttrain-rmse:3237.09546\n",
      "[99]\teval-rmse:3979.31958\ttrain-rmse:3236.22241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "params_tree = {\n",
    "    'nthread': 1,\n",
    "    'objective':'reg:squarederror',\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "\n",
    "params_linear = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "\n",
    "\n",
    "rounds = 100\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "test_x, test_y = df_test.iloc[:, 1:], df_test.iloc[:, :1]\n",
    "dtest = xgb.DMatrix(data=test_x,label=test_y)\n",
    "callback_monitor = xgb.callback.EvaluationMonitor(rank=0, period=10, show_stdv=False)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=df_train.iloc[:, 1:], label=df_train.iloc[:, :1])\n",
    "dvalidation = xgb.DMatrix(data=df_validation.iloc[:, 1:], label=df_validation.iloc[:, :1])\n",
    "watchlist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(params_linear, \n",
    "                dtrain, \n",
    "                rounds,\n",
    "                watchlist,\n",
    "                early_stopping_rounds=20,      \n",
    "                callbacks=[callback_monitor],\n",
    "                verbose_eval=False)\n",
    "bst.save_model('temp/predictor.model')\n",
    "predictions = bst.predict(dtest)\n",
    "score = mean_squared_error(test_y, predictions, squared=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the Prescription Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212400 entries, 0 to 212399\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   PrescriptionIndex                     212400 non-null  int64         \n",
      " 1   CountryName                           212400 non-null  object        \n",
      " 2   RegionName                            50400 non-null   object        \n",
      " 3   Date                                  212400 non-null  datetime64[ns]\n",
      " 4   C1_School closing                     212400 non-null  int64         \n",
      " 5   C2_Workplace closing                  212400 non-null  int64         \n",
      " 6   C3_Cancel public events               212400 non-null  int64         \n",
      " 7   C4_Restrictions on gatherings         212400 non-null  int64         \n",
      " 8   C5_Close public transport             212400 non-null  int64         \n",
      " 9   C6_Stay at home requirements          212400 non-null  int64         \n",
      " 10  C7_Restrictions on internal movement  212400 non-null  int64         \n",
      " 11  C8_International travel controls      212400 non-null  int64         \n",
      " 12  H1_Public information campaigns       212400 non-null  int64         \n",
      " 13  H2_Testing policy                     212400 non-null  int64         \n",
      " 14  H3_Contact tracing                    212400 non-null  int64         \n",
      " 15  H6_Facial Coverings                   212400 non-null  int64         \n",
      " 16  EstimatedCases                        212400 non-null  float64       \n",
      " 17  EstimatedScore                        212400 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(13), object(2)\n",
      "memory usage: 29.2+ MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-09b2c1b26fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PrescriptionIndex'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mdo_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-09b2c1b26fad>\u001b[0m in \u001b[0;36mdo_predictions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PrescriptionIndex'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mdo_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-09b2c1b26fad>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(prescription)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# filter the date range for the prediction window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mdays_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mdate_to_predict_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mdate_to_predict_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_to_predict_from\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays_to_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdf_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3015\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3068\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3597\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3598\u001b[0m         \"\"\"\n\u001b[0;32m-> 3599\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3600\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3585\u001b[0m         new_data = self._mgr.take(\n\u001b[0;32m-> 3586\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3587\u001b[0m         )\n\u001b[1;32m   3588\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"take\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1475\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m         )\n\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                     ),\n\u001b[1;32m   1318\u001b[0m                 )\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             ]\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                     ),\n\u001b[1;32m   1318\u001b[0m                 )\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             ]\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         )\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1757\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m     )\n\u001b[0;32m-> 1759\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_ma(df_x, field, window_size):\n",
    "    df_x[f\"{field}_ma_{window_size}\"] = df_x.groupby('geo_code')[field].rolling(window_size, center=False).mean().fillna(0).reset_index(0, drop=True)\n",
    "\n",
    "\n",
    "def predict_day(\n",
    "        df_predict,\n",
    "        group, \n",
    "        prescription):\n",
    "    \n",
    "    # get the dates, and check to see if we should predict or not\n",
    "    date_from = group['date'].max()\n",
    "    date_to = date_from + pd.to_timedelta(1, unit='d')   \n",
    "    \n",
    "    # get the geo code and values, we'll need to map back\n",
    "    # to the origial data frame for output\n",
    "    group = group.copy()\n",
    "    geo_codes = group['geo_code'].values\n",
    "    \n",
    "    # try applying the prescriptions\n",
    "    if prescription is not None:\n",
    "        for _, row in prescription.loc[prescription['Date'] == date_from].iterrows():        \n",
    "            filter= (group['country_name'] == row['CountryName']) & (roup['region_name'] == row['RegionName'])\n",
    "            group.loc[filter, 'c1_school_closing'] = row['C1_School closing']\n",
    "            #group.loc[filter, 'c2_workplace_closing'] = row['C2_Workplace closing']\n",
    "            #group.loc[filter, 'c3_cancel_public_events'] = row['C3_Cancel public events']\n",
    "            #group.loc[filter, 'c4_restrictions_on_gatherings'] = row['C4_Restrictions on gatherings']\n",
    "            #group.loc[filter, 'c5_close_public_transport'] = row['C5_Close public transport']\n",
    "            #group.loc[filter, 'c6_stay_at_home_requirements'] = row['C6_Stay at home requirements']\n",
    "            #group.loc[filter, 'c7_restrictions_on_internal_movement'] = row['C7_Restrictions on internal movement']\n",
    "            #group.loc[filter, 'c8_international_travel_controls'] = row['C8_International travel controls']            \n",
    "            #group.loc[filter, 'h1_public_information_campaigns'] = row['H1_Public information campaigns']            \n",
    "            #group.loc[filter, 'h2_testing_policy'] = row['H2_Testing policy']            \n",
    "            #group.loc[filter, 'h3_contact_tracing'] = row['H3_Contact tracing']            \n",
    "            #group.loc[filter, 'h6_facial_coverings'] = row['H6_Facial Coverings']            \n",
    "        \n",
    "\n",
    "    \n",
    "    # determine the date we are predicting from encode and scale    \n",
    "    group = encode(group, False)\n",
    "    group = group.drop(labels=['date'], axis=1)\n",
    "    for scaler_name in scalers:\n",
    "        group[scaler_name] = scalers[scaler_name].transform(group[[scaler_name]])\n",
    "\n",
    "    # run the prediction algorithm\n",
    "    print(f\"predicting from {date_from} for {date_to}\")\n",
    "    gx, gy = group.iloc[:, 1:], group.iloc[:, :1]\n",
    "    dg = xgb.DMatrix(data=gx,label=gy)\n",
    "    predictions = bst.predict(dg)\n",
    "\n",
    "    # apply the predictions to the NEXT day\n",
    "    for i in range(len(predictions)):\n",
    "        geo_code = geo_codes[i]\n",
    "        value = max(0, predictions[i])\n",
    "        filter_from = (df_predict['geo_code'] == geo_code) & (df_predict['date'] == date_from)        \n",
    "        filter_to = (df_predict['geo_code'] == geo_code) & (df_predict['date'] == date_to)        \n",
    "        df_predict.loc[filter_to, 'new_cases'] = value\n",
    "        df_predict.loc[filter_to, 'confirmed_cases'] = df_predict.loc[filter_from]['confirmed_cases'].max() + value\n",
    "    \n",
    "    # update the moving averages\n",
    "    for window_size in [3, 7, 21]:\n",
    "        compute_ma(df_predict, 'new_cases', window_size)\n",
    "        compute_ma(df_predict, 'confirmed_cases', window_size)\n",
    "        compute_ma(df_predict, 'c1_school_closing', window_size)        \n",
    "        compute_ma(df_predict, 'c2_workplace_closing', window_size)        \n",
    "        compute_ma(df_predict, 'c3_cancel_public_events', window_size)        \n",
    "        compute_ma(df_predict, 'c4_restrictions_on_gatherings', window_size)   \n",
    "        compute_ma(df_predict, 'c5_close_public_transport', window_size)        \n",
    "        compute_ma(df_predict, 'c6_stay_at_home_requirements', window_size)        \n",
    "        compute_ma(df_predict, 'c7_restrictions_on_internal_movement', window_size)        \n",
    "        compute_ma(df_predict, 'c8_international_travel_controls', window_size)   \n",
    "        compute_ma(df_predict, 'h1_public_information_campaigns', window_size)        \n",
    "        compute_ma(df_predict, 'h2_testing_policy', window_size)        \n",
    "        compute_ma(df_predict, 'h3_contact_tracing', window_size)        \n",
    "        compute_ma(df_predict, 'h6_facial_coverings', window_size)     \n",
    "        compute_ma(df_predict, 'new_cases_as_percent_of_population', window_size)     \n",
    "        compute_ma(df_predict, 'confirmed_cases_as_percent_of_population', window_size)     \n",
    "\n",
    "\n",
    "def predict(prescription):\n",
    "    # filter the date range for the prediction window\n",
    "    days_to_predict = 3\n",
    "    date_to_predict_from = df[df['predicted'] == False]['date'].max()\n",
    "    date_to_predict_to = date_to_predict_from + pd.to_timedelta(days_to_predict, unit='d')   \n",
    "    df_predict = df.copy()\n",
    "    df_predict = df_predict.sort_values(['date', 'geo_code'])\n",
    "    df_predict = df_predict[df_predict['date'] <= pd.to_datetime(date_to_predict_to)]\n",
    "\n",
    "    # predict out the dates\n",
    "    df_predict_filter = (df_predict['date'] >= date_to_predict_from)\n",
    "    df_predict.loc[df_predict_filter].groupby('date').apply(\n",
    "        lambda group: predict_day(\n",
    "            df_predict,\n",
    "            group, \n",
    "            prescription))\n",
    "\n",
    "# read the prescriptions and generate a prediction file for each prescription\n",
    "def do_predictions():\n",
    "    prescriptions = pd.read_csv('prescriptions.csv', keep_default_na=False, na_values='')\n",
    "    prescriptions['Date'] = pd.to_datetime(prescriptions['Date']) \n",
    "    prescriptions.info()\n",
    "    for i in range(10):\n",
    "        print(f\"generating predictions for index {i}\")\n",
    "        predict(prescriptions.loc[prescriptions['PrescriptionIndex'] == i])\n",
    "\n",
    "do_predictions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.loc[df_predict['geo_code'] == 'DE'][['date', 'new_cases', 'confirmed_cases']].tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
