{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Collecting git+https://github.com/rbilleci/pandora.git\n",
      "  Cloning https://github.com/rbilleci/pandora.git to /tmp/pip-req-build-rsfc7_wp\n",
      "  Running command git clone -q https://github.com/rbilleci/pandora.git /tmp/pip-req-build-rsfc7_wp\n",
      "Requirement already satisfied (use --upgrade to upgrade): pandora==0.1.0 from git+https://github.com/rbilleci/pandora.git in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: pandas~=1.2.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: fnvhash~=0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn~=0.24.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.24.1)\n",
      "Requirement already satisfied: workalendar~=14.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: category-encoders~=2.2.2 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: pyCalverter in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: skyfield in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.36)\n",
      "Requirement already satisfied: lunardate in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: skyfield-data in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pyluach in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: setuptools>=1.0 in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas~=1.2.1->pandora==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2019.11.28)\n",
      "Requirement already satisfied: sgp4>=2.2 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Requirement already satisfied: jplephem>=2.13 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Building wheels for collected packages: pandora\n",
      "  Building wheel for pandora (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandora: filename=pandora-0.1.0-py3-none-any.whl size=2681412 sha256=120d54e423f81eed34930b641a9b812ab006267ba82c501f53b465521b01f4b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b8nfv62n/wheels/01/8b/d5/a72c927a738750e04a4bb4fd22f63b4b88c7b5871732e2d67b\n",
      "Successfully built pandora\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install git+https://github.com/rbilleci/pandora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from pandora import loader, encoders\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138768 entries, 0 to 138767\n",
      "Data columns (total 107 columns):\n",
      " #    Column                                      Non-Null Count   Dtype         \n",
      "---   ------                                      --------------   -----         \n",
      " 0    predicted_new_cases                         138768 non-null  float64       \n",
      " 1    age_distribution_00_04                      138768 non-null  float64       \n",
      " 2    age_distribution_05_14                      138768 non-null  float64       \n",
      " 3    age_distribution_15_34                      138768 non-null  float64       \n",
      " 4    age_distribution_34_64                      138768 non-null  float64       \n",
      " 5    age_distribution_65_plus                    138768 non-null  float64       \n",
      " 6    c1_school_closing                           138768 non-null  float64       \n",
      " 7    c1_school_closing_ma_21                     138768 non-null  float64       \n",
      " 8    c1_school_closing_ma_3                      138768 non-null  float64       \n",
      " 9    c1_school_closing_ma_7                      138768 non-null  float64       \n",
      " 10   c2_workplace_closing                        138768 non-null  float64       \n",
      " 11   c2_workplace_closing_ma_21                  138768 non-null  float64       \n",
      " 12   c2_workplace_closing_ma_3                   138768 non-null  float64       \n",
      " 13   c2_workplace_closing_ma_7                   138768 non-null  float64       \n",
      " 14   c3_cancel_public_events                     138768 non-null  float64       \n",
      " 15   c3_cancel_public_events_ma_21               138768 non-null  float64       \n",
      " 16   c3_cancel_public_events_ma_3                138768 non-null  float64       \n",
      " 17   c3_cancel_public_events_ma_7                138768 non-null  float64       \n",
      " 18   c4_restrictions_on_gatherings               138768 non-null  float64       \n",
      " 19   c4_restrictions_on_gatherings_ma_21         138768 non-null  float64       \n",
      " 20   c4_restrictions_on_gatherings_ma_3          138768 non-null  float64       \n",
      " 21   c4_restrictions_on_gatherings_ma_7          138768 non-null  float64       \n",
      " 22   c5_close_public_transport                   138768 non-null  float64       \n",
      " 23   c5_close_public_transport_ma_21             138768 non-null  float64       \n",
      " 24   c5_close_public_transport_ma_3              138768 non-null  float64       \n",
      " 25   c5_close_public_transport_ma_7              138768 non-null  float64       \n",
      " 26   c6_stay_at_home_requirements                138768 non-null  float64       \n",
      " 27   c6_stay_at_home_requirements_ma_21          138768 non-null  float64       \n",
      " 28   c6_stay_at_home_requirements_ma_3           138768 non-null  float64       \n",
      " 29   c6_stay_at_home_requirements_ma_7           138768 non-null  float64       \n",
      " 30   c7_restrictions_on_internal_movement        138768 non-null  float64       \n",
      " 31   c7_restrictions_on_internal_movement_ma_21  138768 non-null  float64       \n",
      " 32   c7_restrictions_on_internal_movement_ma_3   138768 non-null  float64       \n",
      " 33   c7_restrictions_on_internal_movement_ma_7   138768 non-null  float64       \n",
      " 34   c8_international_travel_controls            138768 non-null  float64       \n",
      " 35   c8_international_travel_controls_ma_21      138768 non-null  float64       \n",
      " 36   c8_international_travel_controls_ma_3       138768 non-null  float64       \n",
      " 37   c8_international_travel_controls_ma_7       138768 non-null  float64       \n",
      " 38   confirmed_cases                             138768 non-null  float64       \n",
      " 39   confirmed_cases--                           138768 non-null  bool          \n",
      " 40   confirmed_cases_as_percent_of_population    138768 non-null  float64       \n",
      " 41   confirmed_cases_ma_21                       138768 non-null  float64       \n",
      " 42   confirmed_cases_ma_3                        138768 non-null  float64       \n",
      " 43   confirmed_cases_ma_7                        138768 non-null  float64       \n",
      " 44   confirmed_deaths                            138768 non-null  float64       \n",
      " 45   continent                                   138768 non-null  object        \n",
      " 46   country_code                                138768 non-null  object        \n",
      " 47   country_code3                               138768 non-null  object        \n",
      " 48   country_code_numeric                        138768 non-null  int64         \n",
      " 49   country_name                                138768 non-null  object        \n",
      " 50   date                                        138768 non-null  datetime64[ns]\n",
      " 51   day_of_month                                138768 non-null  int64         \n",
      " 52   day_of_week                                 138768 non-null  int64         \n",
      " 53   day_of_year                                 138768 non-null  int64         \n",
      " 54   gdp_per_capita                              138768 non-null  float64       \n",
      " 55   gdp_per_capita--                            138768 non-null  bool          \n",
      " 56   geo_code                                    138768 non-null  object        \n",
      " 57   h1_public_information_campaigns             138768 non-null  float64       \n",
      " 58   h1_public_information_campaigns_ma_21       138768 non-null  float64       \n",
      " 59   h1_public_information_campaigns_ma_3        138768 non-null  float64       \n",
      " 60   h1_public_information_campaigns_ma_7        138768 non-null  float64       \n",
      " 61   h2_testing_policy                           138768 non-null  float64       \n",
      " 62   h2_testing_policy_ma_21                     138768 non-null  float64       \n",
      " 63   h2_testing_policy_ma_3                      138768 non-null  float64       \n",
      " 64   h2_testing_policy_ma_7                      138768 non-null  float64       \n",
      " 65   h3_contact_tracing                          138768 non-null  float64       \n",
      " 66   h3_contact_tracing_ma_21                    138768 non-null  float64       \n",
      " 67   h3_contact_tracing_ma_3                     138768 non-null  float64       \n",
      " 68   h3_contact_tracing_ma_7                     138768 non-null  float64       \n",
      " 69   h6_facial_coverings                         138768 non-null  float64       \n",
      " 70   h6_facial_coverings_ma_21                   138768 non-null  float64       \n",
      " 71   h6_facial_coverings_ma_3                    138768 non-null  float64       \n",
      " 72   h6_facial_coverings_ma_7                    138768 non-null  float64       \n",
      " 73   month                                       138768 non-null  int64         \n",
      " 74   new_cases                                   138768 non-null  float64       \n",
      " 75   new_cases_as_percent_of_population          138768 non-null  float64       \n",
      " 76   new_cases_ma_21                             138768 non-null  float64       \n",
      " 77   new_cases_ma_3                              138768 non-null  float64       \n",
      " 78   new_cases_ma_7                              138768 non-null  float64       \n",
      " 79   npi_sum                                     138768 non-null  float64       \n",
      " 80   obesity_rate                                138768 non-null  float64       \n",
      " 81   obesity_rate--                              138768 non-null  bool          \n",
      " 82   pneumonia_deaths_per_100k                   138768 non-null  float64       \n",
      " 83   pneumonia_deaths_per_100k--                 138768 non-null  bool          \n",
      " 84   population                                  138768 non-null  float64       \n",
      " 85   population--                                138768 non-null  bool          \n",
      " 86   population_density                          138768 non-null  float64       \n",
      " 87   population_density--                        138768 non-null  bool          \n",
      " 88   population_percent_urban                    138768 non-null  float64       \n",
      " 89   population_percent_urban--                  138768 non-null  bool          \n",
      " 90   predicted                                   138768 non-null  bool          \n",
      " 91   quarter                                     138768 non-null  int64         \n",
      " 92   region_name                                 138768 non-null  object        \n",
      " 93   specific_humidity                           138768 non-null  float64       \n",
      " 94   specific_humidity_ma_21                     138768 non-null  float64       \n",
      " 95   specific_humidity_ma_3                      138768 non-null  float64       \n",
      " 96   specific_humidity_ma_7                      138768 non-null  float64       \n",
      " 97   temperature                                 138768 non-null  float64       \n",
      " 98   week                                        138768 non-null  int64         \n",
      " 99   working_day                                 138768 non-null  float64       \n",
      " 100  working_day--                               138768 non-null  bool          \n",
      " 101  working_day_ma_21                           138768 non-null  float64       \n",
      " 102  working_day_ma_3                            138768 non-null  float64       \n",
      " 103  working_day_ma_7                            138768 non-null  float64       \n",
      " 104  working_day_tomorrow                        138768 non-null  float64       \n",
      " 105  working_day_yesterday                       138768 non-null  float64       \n",
      " 106  year                                        138768 non-null  int64         \n",
      "dtypes: bool(9), datetime64[ns](1), float64(83), int64(8), object(6)\n",
      "memory usage: 104.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (92) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and set the date column\n",
    "df = pd.read_csv('temp/01-data.csv', keep_default_na=False, na_values='')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['region_name'] = df['region_name'].fillna('')\n",
    "\n",
    "# determine the date where prediction should begin\n",
    "prediction_start_date = df[df['predicted'] == True]['date'].min().date()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# declare the encoders\n",
    "enc = {}\n",
    "enc['continent'] = encoders.BinaryEncoder('continent')\n",
    "enc['geo_code'] = encoders.BinaryEncoder('geo_code')\n",
    "enc['country_code'] = encoders.BinaryEncoder('country_code')\n",
    "enc['day_of_week'] = encoders.BinaryEncoder('day_of_week')\n",
    "enc['day_of_week_cyc'] = encoders.CyclicalEncoder('day_of_week')\n",
    "enc['day_of_month'] = encoders.BinaryEncoder('day_of_month')\n",
    "enc['day_of_month_cyc'] = encoders.CyclicalEncoder('day_of_month')\n",
    "enc['day_of_year'] = encoders.BinaryEncoder('day_of_year')\n",
    "enc['day_of_year_cyc'] = encoders.CyclicalEncoder('day_of_year')\n",
    "\n",
    "def encode(df_x, fit):\n",
    "    \n",
    "    # encode the geo data\n",
    "    if fit:\n",
    "        df_x = enc['continent'].fit_transform(df_x)\n",
    "        df_x = enc['geo_code'].fit_transform(df_x)\n",
    "        df_x = enc['country_code'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['continent'].transform(df_x)\n",
    "        df_x = enc['geo_code'].transform(df_x)\n",
    "        df_x = enc['country_code'].transform(df_x)\n",
    "        \n",
    "    if fit:\n",
    "        df_x = enc['day_of_week'].fit_transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'].apply(lambda x: x / 7.)\n",
    "        df_x = enc['day_of_week_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['day_of_week'].transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'].apply(lambda x: x / 7.)\n",
    "        df_x = enc['day_of_week_cyc'].transform(df_x)\n",
    "        \n",
    "    if fit:\n",
    "        df_x['day_of_month'] = df_x['day_of_month'].apply(lambda x: x / 31.)\n",
    "        df_x = enc['day_of_month_cyc'].fit_transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'].apply(lambda x: x / 366.)\n",
    "        df_x = enc['day_of_year_cyc'].fit_transform(df_x)\n",
    "    else:    \n",
    "        df_x['day_of_month'] = df_x['day_of_month'].apply(lambda x: x / 31.)\n",
    "        df_x = enc['day_of_month_cyc'].transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'].apply(lambda x: x / 366.)\n",
    "        df_x = enc['day_of_year_cyc'].transform(df_x)\n",
    "    \n",
    "    # convert the date to an integer value\n",
    "    df_x['date_day'] = df_x['date'].apply(lambda x: x.day)\n",
    "    \n",
    "    # drop unused columns\n",
    "    df_x = df_x.drop(labels=['country_name',\n",
    "                           'continent',\n",
    "                           'country_code',\n",
    "                           'day_of_week',\n",
    "                           'day_of_month',\n",
    "                           'day_of_year',\n",
    "                            'npi_sum',\n",
    "                            'pneumonia_deaths_per_100k',\n",
    "                            'pneumonia_deaths_per_100k--',\n",
    "                            'country_code3',\n",
    "                            'country_code_numeric',\n",
    "                            'confirmed_deaths',\n",
    "                            'predicted',\n",
    "                            'region_name',\n",
    "                            'month',                           \n",
    "                            'quarter',\n",
    "                            'week',\n",
    "                            'temperature',\n",
    "                            'year'], axis=1)\n",
    "    return df_x\n",
    "\n",
    "# only work within the specified range\n",
    "df_ml = df.loc[df['date'] < pd.to_datetime(prediction_start_date)]\n",
    "df_ml = encode(df_ml, fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train, val, test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Range:   2020-01-01 - 2021-01-06\n",
      "Validation Range: 2021-01-07 - 2021-01-27\n",
      "Test Range:       2021-01-28 - 2021-02-10\n"
     ]
    }
   ],
   "source": [
    "days_for_validation = 21\n",
    "days_for_test = 14\n",
    "\n",
    "def split(df: pd.DataFrame, \n",
    "          days_for_validation: int, \n",
    "          days_for_test: int) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # First, sort the data by date\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Determine the maximum date\n",
    "    date_start_test = df['date'].max() - pd.to_timedelta(days_for_test - 1, unit='d')\n",
    "    date_start_validation = date_start_test - pd.to_timedelta(days_for_validation, unit='d')\n",
    "\n",
    "    df_train = df[df['date'] < date_start_validation]\n",
    "    df_validation = df[(df['date'] >= date_start_validation) & (df['date'] < date_start_test)]\n",
    "    df_test = df[df['date'] >= date_start_test]\n",
    "\n",
    "    # Debug the outpoint\n",
    "    print(f\"Training Range:   {df_train['date'].min().date()} - {df_train['date'].max().date()}\")\n",
    "    print(f\"Validation Range: {df_validation['date'].min().date()} - {df_validation['date'].max().date()}\")\n",
    "    print(f\"Test Range:       {df_test['date'].min().date()} - {df_test['date'].max().date()}\")\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(df.index) != len(df_train.index) + len(df_validation.index) + len(df_test.index):\n",
    "        raise Exception('entries do not add up')\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "df_train_prescaled, df_validation_prescaled, df_test_prescaled = split(df_ml, days_for_validation, days_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_prescaled.copy()\n",
    "df_validation = df_validation_prescaled.copy()\n",
    "df_test = df_test_prescaled.copy()\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "for feature_name in df_ml.columns.values:\n",
    "    if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "        continue\n",
    "    scalers[feature_name] = StandardScaler()\n",
    "    df_train[feature_name] = scalers[feature_name].fit_transform(df_train_prescaled[[feature_name]])\n",
    "        \n",
    "if len(df_validation_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue\n",
    "        df_validation[feature_name] = scalers[feature_name].transform(df_validation_prescaled[[feature_name]])\n",
    "\n",
    "if len(df_test_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue        \n",
    "        df_test[feature_name] = scalers[feature_name].transform(df_test_prescaled[[feature_name]])\n",
    "\n",
    "df_train = df_train.drop(labels=['geo_code', 'date'], axis=1)\n",
    "df_validation = df_validation.drop(labels=['geo_code', 'date'], axis=1)\n",
    "df_test = df_test.drop(labels=['geo_code', 'date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:8251.42188\ttrain-rmse:4282.09522\n",
      "[10]\teval-rmse:3946.68311\ttrain-rmse:3375.67920\n",
      "[20]\teval-rmse:3737.02148\ttrain-rmse:3313.60913\n",
      "[30]\teval-rmse:3768.69971\ttrain-rmse:3294.04102\n",
      "[40]\teval-rmse:3790.04639\ttrain-rmse:3286.70752\n",
      "[50]\teval-rmse:3795.46240\ttrain-rmse:3283.09448\n",
      "[60]\teval-rmse:3791.55029\ttrain-rmse:3280.67554\n",
      "[70]\teval-rmse:3782.72803\ttrain-rmse:3278.71387\n",
      "[80]\teval-rmse:3771.71826\ttrain-rmse:3276.99976\n",
      "[90]\teval-rmse:3760.03516\ttrain-rmse:3275.47559\n",
      "[100]\teval-rmse:3748.52002\ttrain-rmse:3274.10840\n",
      "[110]\teval-rmse:3737.57812\ttrain-rmse:3272.89258\n",
      "[120]\teval-rmse:3727.39111\ttrain-rmse:3271.82056\n",
      "[130]\teval-rmse:3718.01318\ttrain-rmse:3270.86963\n",
      "[140]\teval-rmse:3709.42895\ttrain-rmse:3270.02783\n",
      "[150]\teval-rmse:3701.60303\ttrain-rmse:3269.29224\n",
      "[160]\teval-rmse:3694.46558\ttrain-rmse:3268.64331\n",
      "[170]\teval-rmse:3687.96729\ttrain-rmse:3268.07397\n",
      "[180]\teval-rmse:3682.04492\ttrain-rmse:3267.57275\n",
      "[190]\teval-rmse:3676.64526\ttrain-rmse:3267.13379\n",
      "[200]\teval-rmse:3671.71802\ttrain-rmse:3266.74878\n",
      "[210]\teval-rmse:3667.21362\ttrain-rmse:3266.40771\n",
      "[220]\teval-rmse:3663.09644\ttrain-rmse:3266.11548\n",
      "[230]\teval-rmse:3659.32178\ttrain-rmse:3265.85693\n",
      "[240]\teval-rmse:3655.86645\ttrain-rmse:3265.63159\n",
      "[250]\teval-rmse:3652.69214\ttrain-rmse:3265.43164\n",
      "[260]\teval-rmse:3649.77881\ttrain-rmse:3265.26074\n",
      "[270]\teval-rmse:3647.09985\ttrain-rmse:3265.10840\n",
      "[280]\teval-rmse:3644.63745\ttrain-rmse:3264.97754\n",
      "[290]\teval-rmse:3642.37085\ttrain-rmse:3264.85889\n",
      "[300]\teval-rmse:3640.27881\ttrain-rmse:3264.76172\n",
      "[310]\teval-rmse:3638.35034\ttrain-rmse:3264.67334\n",
      "[320]\teval-rmse:3636.57422\ttrain-rmse:3264.59497\n",
      "[330]\teval-rmse:3634.93457\ttrain-rmse:3264.52832\n",
      "[340]\teval-rmse:3633.41528\ttrain-rmse:3264.46924\n",
      "[350]\teval-rmse:3632.01294\ttrain-rmse:3264.41870\n",
      "[360]\teval-rmse:3630.71729\ttrain-rmse:3264.37305\n",
      "[370]\teval-rmse:3629.52002\ttrain-rmse:3264.33325\n",
      "[380]\teval-rmse:3628.40894\ttrain-rmse:3264.29810\n",
      "[390]\teval-rmse:3627.38062\ttrain-rmse:3264.26807\n",
      "[400]\teval-rmse:3626.42822\ttrain-rmse:3264.24023\n",
      "[410]\teval-rmse:3625.54663\ttrain-rmse:3264.21851\n",
      "[420]\teval-rmse:3624.73120\ttrain-rmse:3264.19727\n",
      "[430]\teval-rmse:3623.97217\ttrain-rmse:3264.17871\n",
      "[440]\teval-rmse:3623.27002\ttrain-rmse:3264.16260\n",
      "[450]\teval-rmse:3622.61865\ttrain-rmse:3264.14844\n",
      "[460]\teval-rmse:3622.01660\ttrain-rmse:3264.13428\n",
      "[470]\teval-rmse:3621.45703\ttrain-rmse:3264.12500\n",
      "[480]\teval-rmse:3620.93701\ttrain-rmse:3264.11401\n",
      "[490]\teval-rmse:3620.45435\ttrain-rmse:3264.10620\n",
      "[500]\teval-rmse:3620.00806\ttrain-rmse:3264.09717\n",
      "[510]\teval-rmse:3619.59204\ttrain-rmse:3264.08862\n",
      "[520]\teval-rmse:3619.20532\ttrain-rmse:3264.07983\n",
      "[530]\teval-rmse:3618.84595\ttrain-rmse:3264.07495\n",
      "[540]\teval-rmse:3618.51538\ttrain-rmse:3264.06836\n",
      "[550]\teval-rmse:3618.20654\ttrain-rmse:3264.06201\n",
      "[560]\teval-rmse:3617.92090\ttrain-rmse:3264.06006\n",
      "[570]\teval-rmse:3617.65479\ttrain-rmse:3264.05566\n",
      "[580]\teval-rmse:3617.40747\ttrain-rmse:3264.05005\n",
      "[590]\teval-rmse:3617.17969\ttrain-rmse:3264.04663\n",
      "[600]\teval-rmse:3616.96484\ttrain-rmse:3264.04248\n",
      "[610]\teval-rmse:3616.76611\ttrain-rmse:3264.03760\n",
      "[620]\teval-rmse:3616.58350\ttrain-rmse:3264.03589\n",
      "[630]\teval-rmse:3616.41309\ttrain-rmse:3264.03516\n",
      "[640]\teval-rmse:3616.25391\ttrain-rmse:3264.03223\n",
      "[650]\teval-rmse:3616.10815\ttrain-rmse:3264.02954\n",
      "[660]\teval-rmse:3615.97339\ttrain-rmse:3264.02563\n",
      "[670]\teval-rmse:3615.84619\ttrain-rmse:3264.02515\n",
      "[680]\teval-rmse:3615.73193\ttrain-rmse:3264.02246\n",
      "[690]\teval-rmse:3615.62256\ttrain-rmse:3264.02002\n",
      "[700]\teval-rmse:3615.52124\ttrain-rmse:3264.01880\n",
      "[710]\teval-rmse:3615.42993\ttrain-rmse:3264.01660\n",
      "[720]\teval-rmse:3615.34644\ttrain-rmse:3264.01562\n",
      "[730]\teval-rmse:3615.26440\ttrain-rmse:3264.01367\n",
      "[740]\teval-rmse:3615.18726\ttrain-rmse:3264.01123\n",
      "[750]\teval-rmse:3615.12012\ttrain-rmse:3264.00928\n",
      "[760]\teval-rmse:3615.05786\ttrain-rmse:3264.00855\n",
      "[770]\teval-rmse:3614.99927\ttrain-rmse:3264.00879\n",
      "[780]\teval-rmse:3614.94312\ttrain-rmse:3264.00635\n",
      "[790]\teval-rmse:3614.89307\ttrain-rmse:3264.00391\n",
      "[800]\teval-rmse:3614.84619\ttrain-rmse:3264.00269\n",
      "[810]\teval-rmse:3614.80566\ttrain-rmse:3264.00098\n",
      "[820]\teval-rmse:3614.76782\ttrain-rmse:3263.99902\n",
      "[830]\teval-rmse:3614.72900\ttrain-rmse:3263.99609\n",
      "[840]\teval-rmse:3614.69605\ttrain-rmse:3263.99438\n",
      "[850]\teval-rmse:3614.66602\ttrain-rmse:3263.99316\n",
      "[860]\teval-rmse:3614.63818\ttrain-rmse:3263.99194\n",
      "[870]\teval-rmse:3614.61255\ttrain-rmse:3263.98999\n",
      "[880]\teval-rmse:3614.58716\ttrain-rmse:3263.98853\n",
      "[890]\teval-rmse:3614.56421\ttrain-rmse:3263.98804\n",
      "[900]\teval-rmse:3614.54565\ttrain-rmse:3263.98608\n",
      "[910]\teval-rmse:3614.52856\ttrain-rmse:3263.98584\n",
      "[920]\teval-rmse:3614.51147\ttrain-rmse:3263.98413\n",
      "[930]\teval-rmse:3614.49512\ttrain-rmse:3263.98389\n",
      "[940]\teval-rmse:3614.48120\ttrain-rmse:3263.98242\n",
      "[950]\teval-rmse:3614.46802\ttrain-rmse:3263.98047\n",
      "[960]\teval-rmse:3614.45703\ttrain-rmse:3263.97900\n",
      "[970]\teval-rmse:3614.44800\ttrain-rmse:3263.97705\n",
      "[980]\teval-rmse:3614.43774\ttrain-rmse:3263.97681\n",
      "[990]\teval-rmse:3614.43188\ttrain-rmse:3263.97461\n",
      "[999]\teval-rmse:3614.42603\ttrain-rmse:3263.97363\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "params_tree = {\n",
    "    'nthread': 1,\n",
    "    'objective':'reg:squarederror',\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "\n",
    "params_linear = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "\n",
    "\n",
    "rounds = 1000\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "test_x, test_y = df_test.iloc[:, 1:], df_test.iloc[:, :1]\n",
    "dtest = xgb.DMatrix(data=test_x,label=test_y)\n",
    "callback_monitor = xgb.callback.EvaluationMonitor(rank=0, period=10, show_stdv=False)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=df_train.iloc[:, 1:], label=df_train.iloc[:, :1])\n",
    "dvalidation = xgb.DMatrix(data=df_validation.iloc[:, 1:], label=df_validation.iloc[:, :1])\n",
    "watchlist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(params_linear, \n",
    "                dtrain, \n",
    "                rounds,\n",
    "                watchlist,\n",
    "                early_stopping_rounds=20,      \n",
    "                callbacks=[callback_monitor],\n",
    "                verbose_eval=False)\n",
    "bst.save_model('temp/predictor.model')\n",
    "predictions = bst.predict(dtest)\n",
    "score = mean_squared_error(test_y, predictions, squared=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the Prescription Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 00:30:42.021873 - evaluating baselin plan\n",
      "2021-02-12 00:30:42.165762 - predicting\n",
      "2021-02-12 00:54:43.605991 - generating predictions for index 0\n",
      "2021-02-12 00:54:43.619722 - applying intervention plan\n",
      "2021-02-12 00:55:37.741257 - predicting\n",
      "2021-02-12 01:20:04.268035 - generating predictions for index 1\n",
      "2021-02-12 01:20:04.271597 - applying intervention plan\n",
      "2021-02-12 01:21:00.143422 - predicting\n",
      "2021-02-12 01:45:57.392249 - generating predictions for index 2\n",
      "2021-02-12 01:45:57.396042 - applying intervention plan\n",
      "2021-02-12 01:46:53.479173 - predicting\n",
      "2021-02-12 02:11:51.268466 - generating predictions for index 3\n",
      "2021-02-12 02:11:51.271888 - applying intervention plan\n",
      "2021-02-12 02:12:47.260838 - predicting\n",
      "2021-02-12 02:36:54.456971 - generating predictions for index 4\n",
      "2021-02-12 02:36:54.460503 - applying intervention plan\n",
      "2021-02-12 02:37:48.969809 - predicting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_geo_date_index(df_x):\n",
    "    df_x['_index'] = df_x['CountryName'] + df_x['RegionName'] + df_x['Date'].dt.strftime('%Y%m%d')\n",
    "    return df_x\n",
    "    \n",
    "\n",
    "# read the prescriptions and generate a prediction file for each prescription\n",
    "def evaluate_intervention_plans(days):\n",
    "    df_intervention_plans = pd.read_csv('prescriptions.csv', keep_default_na=False, na_values='')\n",
    "    df_intervention_plans['RegionName'] = df_intervention_plans['RegionName'].fillna('')\n",
    "    df_intervention_plans['Date'] = pd.to_datetime(df_intervention_plans['Date']) \n",
    "    df_intervention_plans = add_geo_date_index(df_intervention_plans)\n",
    "    for i in range(10):\n",
    "        print(f\"{datetime.datetime.now()} - generating predictions for index {i}\")\n",
    "        df_intervention_plan = df_intervention_plans.loc[df_intervention_plans['PrescriptionIndex'] == i]\n",
    "        evaluate_intervention_plan(i, df_intervention_plan, days)\n",
    "\n",
    "def evaluate_intervention_plan(prescription_index, \n",
    "                               df_intervention_plan, \n",
    "                               days_to_predict):\n",
    "    print(f\"{datetime.datetime.now()} - applying intervention plan\")\n",
    "    date_to_predict_from = df[df['predicted'] == False]['date'].max()\n",
    "    date_to_predict_to = date_to_predict_from + pd.to_timedelta(days_to_predict, unit='d')  \n",
    "    df_out = df.copy()\n",
    "    df_out = df_out[df_out['date'] <= pd.to_datetime(date_to_predict_to) + pd.to_timedelta(1, unit='d')  ]\n",
    "    df_out = df_out.sort_values(['geo_code', 'date'])\n",
    "    \n",
    "    for _, row in df_out.loc[df_out['date'] >= date_to_predict_from].iterrows():\n",
    "        date_from = row['date']\n",
    "        country_name = row['country_name']\n",
    "        region_name = row['region_name']\n",
    "        key = country_name + region_name + date_from.strftime('%Y%m%d')\n",
    "        ip_row = df_intervention_plan.loc[df_intervention_plan['_index'] == key] \n",
    "        row['c1_school_closing'] = ip_row['C1_School closing'].max()\n",
    "        row['c2_workplace_closing'] = ip_row['C2_Workplace closing'].max()\n",
    "        row['c3_cancel_public_events'] = ip_row['C3_Cancel public events'].max()\n",
    "        row['c4_restrictions_on_gatherings'] = ip_row['C4_Restrictions on gatherings'].max()\n",
    "        row['c5_close_public_transport'] = ip_row['C5_Close public transport'].max()\n",
    "        row['c6_stay_at_home_requirements'] = ip_row['C6_Stay at home requirements'].max()\n",
    "        row['c7_restrictions_on_internal_movement'] = ip_row['C7_Restrictions on internal movement'].max()\n",
    "        row['c8_international_travel_controls'] = ip_row['C8_International travel controls'].max()\n",
    "        row['h1_public_information_campaigns'] = ip_row['H1_Public information campaigns'].max()\n",
    "        row['h2_testing_policy'] = ip_row['H2_Testing policy'].max()\n",
    "        row['h3_contact_tracing'] = ip_row['H3_Contact tracing'].max()\n",
    "        row['h6_facial_coverings'] = ip_row['H6_Facial Coverings'].max()    \n",
    "    prediction = predict(df_out, date_to_predict_from, date_to_predict_to)\n",
    "    prediction.to_csv(f\"_plan_{prescription_index}.csv\", index=False)\n",
    "\n",
    "def predict_baseline(days_to_predict):\n",
    "    print(f\"{datetime.datetime.now()} - evaluating baselin plan\")\n",
    "    date_to_predict_from = df[df['predicted'] == False]['date'].max()\n",
    "    date_to_predict_to = date_to_predict_from + pd.to_timedelta(days_to_predict, unit='d')  \n",
    "    df_out = df.copy()\n",
    "    df_out = df_out[df_out['date'] <= pd.to_datetime(date_to_predict_to) + pd.to_timedelta(1, unit='d')  ]\n",
    "    df_out = df_out.sort_values(['geo_code', 'date'])        \n",
    "    prediction = predict(df_out, date_to_predict_from, date_to_predict_to)    \n",
    "    prediction.to_csv(f\"_plan_baseline.csv\", index=False)\n",
    "    \n",
    "def predict(df_out, \n",
    "            date_to_predict_from, \n",
    "            date_to_predict_to):\n",
    "    # predict each country\n",
    "    print(f\"{datetime.datetime.now()} - predicting\")\n",
    "    df_out = df_out.groupby(['geo_code']).apply(\n",
    "        lambda g: predict_for_geo(g, date_to_predict_from, date_to_predict_to)).reset_index(0, drop=True)\n",
    "    \n",
    "    # filter out any extra days\n",
    "    df_out = df_out[df_out['date'] <= pd.to_datetime(date_to_predict_to)]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def predict_for_geo(df_geo,\n",
    "                    date_to_predict_from, \n",
    "                    date_to_predict_to):\n",
    "    geo_code = df_geo['geo_code'].iloc[0]\n",
    "    df_input = df_geo.copy()\n",
    "    df_input = encode(df_input, fit=False)\n",
    "\n",
    "    # scale values\n",
    "    for name in scalers:\n",
    "        if name not in df_input.columns:\n",
    "            continue\n",
    "        if name == 'confirmed_cases':\n",
    "            continue\n",
    "        if name == 'new_cases':\n",
    "            continue\n",
    "        df_input[name] = scalers[name].transform(df_input[[name]])\n",
    "    \n",
    "    # get iterators we'll use on the rows\n",
    "    df_geo_it = df_geo.loc[df_geo['date'] >= date_to_predict_from].iterrows()\n",
    "    df_input_it = df_input.loc[df_input['date'] >= date_to_predict_from].iterrows()\n",
    "    \n",
    "    # predict each day\n",
    "    new_cases = 0.\n",
    "    new_cases_ma = []\n",
    "    confirmed_cases = 0.\n",
    "    confirmed_cases_ma = []\n",
    "    first_row = True\n",
    "    \n",
    "    for x, row in df_geo_it:\n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            new_cases = row['new_cases']\n",
    "            new_cases_ma = [new_cases] * 21\n",
    "            confirmed_cases = row['confirmed_cases']     \n",
    "            confirmed_cases_ma = [confirmed_cases] * 21       \n",
    "        else:            \n",
    "            row['new_cases'] = new_cases\n",
    "            row['new_cases_as_percent_of_population'] = new_cases / row['population']              \n",
    "            row['confirmed_cases'] = confirmed_cases\n",
    "            row['confirmed_cases_as_percent_of_population'] = confirmed_cases / row['population']        \n",
    "        \n",
    "        # update the moving averages for this row\n",
    "        for window_size in [3, 7, 21]:\n",
    "            row[f\"new_cases_ma_{window_size}\"] = np.mean(new_cases_ma[-window_size:])\n",
    "            row[f\"confirmed_cases_ma_{window_size}\"] = np.mean(confirmed_cases_ma[-window_size:])\n",
    "        \n",
    "        # get the dates\n",
    "        date_from = row['date']\n",
    "        date_to = date_from + pd.to_timedelta(1, unit='d')  \n",
    "\n",
    "        # convert to df\n",
    "        model_input = next(df_input_it)[1]    \n",
    "        model_input = pd.DataFrame(model_input.to_frame().T)\n",
    "        model_input['new_cases'] = new_cases\n",
    "        model_input['new_cases_as_percent_of_population'] = row['new_cases_as_percent_of_population']\n",
    "        model_input['confirmed_cases'] = confirmed_cases\n",
    "        model_input['confirmed_cases_as_percent_of_population'] = row['confirmed_cases_as_percent_of_population']\n",
    "        model_input = model_input.drop(labels=['geo_code', 'date'], axis=1)\n",
    "        for name in model_input.columns:\n",
    "            model_input[name] = pd.to_numeric(model_input[name])\n",
    "        model_input['new_cases'] = scalers['new_cases'].transform(model_input[['new_cases']])        \n",
    "        model_input['new_cases_as_percent_of_population'] = scalers['new_cases_as_percent_of_population'].transform(model_input[['new_cases_as_percent_of_population']])        \n",
    "        model_input['confirmed_cases'] = scalers['confirmed_cases'].transform(model_input[['confirmed_cases']])\n",
    "        model_input['confirmed_cases_as_percent_of_population'] = scalers['confirmed_cases_as_percent_of_population'].transform(model_input[['confirmed_cases_as_percent_of_population']])\n",
    "\n",
    "        # predict\n",
    "        predictions = bst.predict(xgb.DMatrix(data=model_input.iloc[:, 1:], label=model_input.iloc[:, :1]))\n",
    "\n",
    "        # update calculations\n",
    "        new_cases = max(0., predictions[0])\n",
    "        confirmed_cases += new_cases\n",
    "        # add to moving averages\n",
    "        new_cases_ma.append(new_cases)\n",
    "        confirmed_cases_ma.append(confirmed_cases) \n",
    "        # assign the row\n",
    "        df_geo.loc[x] = row\n",
    "        \n",
    "    return df_geo\n",
    " \n",
    "\n",
    "predict_baseline(90)\n",
    "evaluate_intervention_plans(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.loc[df_predict['geo_code'] == 'DE'][['date', 'new_cases', 'confirmed_cases']].tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
