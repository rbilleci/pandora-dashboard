{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Collecting git+https://github.com/rbilleci/pandora.git\n",
      "  Cloning https://github.com/rbilleci/pandora.git to /tmp/pip-req-build-rsfc7_wp\n",
      "  Running command git clone -q https://github.com/rbilleci/pandora.git /tmp/pip-req-build-rsfc7_wp\n",
      "Requirement already satisfied (use --upgrade to upgrade): pandora==0.1.0 from git+https://github.com/rbilleci/pandora.git in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: pandas~=1.2.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: fnvhash~=0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn~=0.24.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.24.1)\n",
      "Requirement already satisfied: workalendar~=14.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: category-encoders~=2.2.2 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: pyCalverter in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: skyfield in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.36)\n",
      "Requirement already satisfied: lunardate in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: skyfield-data in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pyluach in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: setuptools>=1.0 in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas~=1.2.1->pandora==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2019.11.28)\n",
      "Requirement already satisfied: sgp4>=2.2 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Requirement already satisfied: jplephem>=2.13 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Building wheels for collected packages: pandora\n",
      "  Building wheel for pandora (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandora: filename=pandora-0.1.0-py3-none-any.whl size=2681412 sha256=120d54e423f81eed34930b641a9b812ab006267ba82c501f53b465521b01f4b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b8nfv62n/wheels/01/8b/d5/a72c927a738750e04a4bb4fd22f63b4b88c7b5871732e2d67b\n",
      "Successfully built pandora\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install git+https://github.com/rbilleci/pandora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from pandora import loader, encoders\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (98) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and set the date column\n",
    "df = pd.read_csv('temp/01-data.csv', keep_default_na=False, na_values='')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# determine the date where prediction should begin\n",
    "prediction_start_date = df[df['predicted'] == True]['date'].min().date()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# declare the encoders\n",
    "enc = {}\n",
    "enc['continent'] = encoders.BinaryEncoder('continent')\n",
    "enc['geo_code'] = encoders.BinaryEncoder('geo_code')\n",
    "enc['country_code'] = encoders.BinaryEncoder('country_code')\n",
    "enc['day_of_week'] = encoders.BinaryEncoder('day_of_week')\n",
    "enc['day_of_week_cyc'] = encoders.CyclicalEncoder('day_of_week')\n",
    "enc['day_of_month'] = encoders.BinaryEncoder('day_of_month')\n",
    "enc['day_of_month_cyc'] = encoders.CyclicalEncoder('day_of_month')\n",
    "enc['day_of_year'] = encoders.BinaryEncoder('day_of_year')\n",
    "enc['day_of_year_cyc'] = encoders.CyclicalEncoder('day_of_year')\n",
    "\n",
    "def encode(df_x, fit):\n",
    "    \n",
    "    # encode the geo data\n",
    "    if fit:\n",
    "        df_x = enc['continent'].fit_transform(df_x)\n",
    "        df_x = enc['geo_code'].fit_transform(df_x)\n",
    "        df_x = enc['country_code'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['continent'].transform(df_x)\n",
    "        df_x = enc['geo_code'].transform(df_x)\n",
    "        df_x = enc['country_code'].transform(df_x)\n",
    "        \n",
    "    if fit:\n",
    "        df_x = enc['day_of_week'].fit_transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'].apply(lambda x: x / 7.)\n",
    "        df_x = enc['day_of_week_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['day_of_week'].transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'].apply(lambda x: x / 7.)\n",
    "        df_x = enc['day_of_week_cyc'].transform(df_x)\n",
    "        \n",
    "    if fit:\n",
    "        df_x['day_of_month'] = df_x['day_of_month'].apply(lambda x: x / 31.)\n",
    "        df_x = enc['day_of_month_cyc'].fit_transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'].apply(lambda x: x / 366.)\n",
    "        df_x = enc['day_of_year_cyc'].fit_transform(df_x)\n",
    "    else:    \n",
    "        df_x['day_of_month'] = df_x['day_of_month'].apply(lambda x: x / 31.)\n",
    "        df_x = enc['day_of_month_cyc'].transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'].apply(lambda x: x / 366.)\n",
    "        df_x = enc['day_of_year_cyc'].transform(df_x)\n",
    "    \n",
    "    # convert the date to an integer value\n",
    "    df_x['date_day'] = df_x['date'].apply(lambda x: x.day)\n",
    "    \n",
    "    # drop unused columns\n",
    "    df_x = df_x.drop(labels=['country_name',\n",
    "                           'continent',\n",
    "                           'country_code',\n",
    "                           'day_of_week',\n",
    "                           'day_of_month',\n",
    "                           'day_of_year',\n",
    "                            'country_code3',\n",
    "                            'country_code_numeric',\n",
    "                            'confirmed_deaths',\n",
    "                            'predicted',\n",
    "                            'region_name',\n",
    "                            'month',                           \n",
    "                            'quarter',\n",
    "                            'week'], axis=1)\n",
    "    return df_x\n",
    "\n",
    "# only work within the specified range\n",
    "df_ml = df.loc[df['date'] < pd.to_datetime(prediction_start_date)]\n",
    "df_ml = encode(df_ml, fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train, val, test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Range:   2020-01-01 - 2020-12-25\n",
      "Validation Range: 2020-12-26 - 2021-01-25\n",
      "Test Range:       2021-01-26 - 2021-02-08\n"
     ]
    }
   ],
   "source": [
    "days_for_validation = 31\n",
    "days_for_test = 14\n",
    "\n",
    "def split(df: pd.DataFrame, \n",
    "          days_for_validation: int, \n",
    "          days_for_test: int) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # First, sort the data by date\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Determine the maximum date\n",
    "    date_start_test = df['date'].max() - pd.to_timedelta(days_for_test - 1, unit='d')\n",
    "    date_start_validation = date_start_test - pd.to_timedelta(days_for_validation, unit='d')\n",
    "\n",
    "    df_train = df[df['date'] < date_start_validation]\n",
    "    df_validation = df[(df['date'] >= date_start_validation) & (df['date'] < date_start_test)]\n",
    "    df_test = df[df['date'] >= date_start_test]\n",
    "\n",
    "    # Debug the outpoint\n",
    "    print(f\"Training Range:   {df_train['date'].min().date()} - {df_train['date'].max().date()}\")\n",
    "    print(f\"Validation Range: {df_validation['date'].min().date()} - {df_validation['date'].max().date()}\")\n",
    "    print(f\"Test Range:       {df_test['date'].min().date()} - {df_test['date'].max().date()}\")\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(df.index) != len(df_train.index) + len(df_validation.index) + len(df_test.index):\n",
    "        raise Exception('entries do not add up')\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "df_train_prescaled, df_validation_prescaled, df_test_prescaled = split(df_ml, days_for_validation, days_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predicted_new_cases' 'age_distribution_00_04' 'age_distribution_05_14'\n",
      " 'age_distribution_15_34' 'age_distribution_34_64'\n",
      " 'age_distribution_65_plus' 'c1_school_closing' 'c1_school_closing_ma_21'\n",
      " 'c1_school_closing_ma_3' 'c1_school_closing_ma_7' 'c2_workplace_closing'\n",
      " 'c2_workplace_closing_ma_21' 'c2_workplace_closing_ma_3'\n",
      " 'c2_workplace_closing_ma_7' 'c3_cancel_public_events'\n",
      " 'c3_cancel_public_events_ma_21' 'c3_cancel_public_events_ma_3'\n",
      " 'c3_cancel_public_events_ma_7' 'c4_restrictions_on_gatherings'\n",
      " 'c4_restrictions_on_gatherings_ma_21'\n",
      " 'c4_restrictions_on_gatherings_ma_3' 'c4_restrictions_on_gatherings_ma_7'\n",
      " 'c5_close_public_transport' 'c5_close_public_transport_ma_21'\n",
      " 'c5_close_public_transport_ma_3' 'c5_close_public_transport_ma_7'\n",
      " 'c6_stay_at_home_requirements' 'c6_stay_at_home_requirements_ma_21'\n",
      " 'c6_stay_at_home_requirements_ma_3' 'c6_stay_at_home_requirements_ma_7'\n",
      " 'c7_restrictions_on_internal_movement'\n",
      " 'c7_restrictions_on_internal_movement_ma_21'\n",
      " 'c7_restrictions_on_internal_movement_ma_3'\n",
      " 'c7_restrictions_on_internal_movement_ma_7'\n",
      " 'c8_international_travel_controls'\n",
      " 'c8_international_travel_controls_ma_21'\n",
      " 'c8_international_travel_controls_ma_3'\n",
      " 'c8_international_travel_controls_ma_7' 'confirmed_cases'\n",
      " 'confirmed_cases--' 'confirmed_cases_as_percent_of_population'\n",
      " 'confirmed_cases_as_percent_of_population_ma_21'\n",
      " 'confirmed_cases_as_percent_of_population_ma_3'\n",
      " 'confirmed_cases_as_percent_of_population_ma_7' 'confirmed_cases_ma_21'\n",
      " 'confirmed_cases_ma_3' 'confirmed_cases_ma_7' 'date' 'gdp_per_capita'\n",
      " 'gdp_per_capita--' 'geo_code' 'h1_public_information_campaigns'\n",
      " 'h1_public_information_campaigns_ma_21'\n",
      " 'h1_public_information_campaigns_ma_3'\n",
      " 'h1_public_information_campaigns_ma_7' 'h2_testing_policy'\n",
      " 'h2_testing_policy_ma_21' 'h2_testing_policy_ma_3'\n",
      " 'h2_testing_policy_ma_7' 'h3_contact_tracing' 'h3_contact_tracing_ma_21'\n",
      " 'h3_contact_tracing_ma_3' 'h3_contact_tracing_ma_7' 'h6_facial_coverings'\n",
      " 'h6_facial_coverings_ma_21' 'h6_facial_coverings_ma_3'\n",
      " 'h6_facial_coverings_ma_7' 'new_cases'\n",
      " 'new_cases_as_percent_of_population'\n",
      " 'new_cases_as_percent_of_population_ma_21'\n",
      " 'new_cases_as_percent_of_population_ma_3'\n",
      " 'new_cases_as_percent_of_population_ma_7' 'new_cases_ma_21'\n",
      " 'new_cases_ma_3' 'new_cases_ma_7' 'npi_sum' 'obesity_rate'\n",
      " 'obesity_rate--' 'pneumonia_deaths_per_100k'\n",
      " 'pneumonia_deaths_per_100k--' 'population' 'population--'\n",
      " 'population_density' 'population_density--' 'population_percent_urban'\n",
      " 'population_percent_urban--' 'specific_humidity'\n",
      " 'specific_humidity_ma_21' 'specific_humidity_ma_3'\n",
      " 'specific_humidity_ma_7' 'temperature' 'temperature_ma_21'\n",
      " 'temperature_ma_3' 'temperature_ma_7' 'working_day' 'working_day--'\n",
      " 'working_day_ma_21' 'working_day_ma_3' 'working_day_ma_7'\n",
      " 'working_day_tomorrow' 'working_day_yesterday' 'year' 'continent_bin_0'\n",
      " 'continent_bin_1' 'continent_bin_2' 'continent_bin_3' 'geo_code_bin_0'\n",
      " 'geo_code_bin_1' 'geo_code_bin_2' 'geo_code_bin_3' 'geo_code_bin_4'\n",
      " 'geo_code_bin_5' 'geo_code_bin_6' 'geo_code_bin_7' 'geo_code_bin_8'\n",
      " 'country_code_bin_0' 'country_code_bin_1' 'country_code_bin_2'\n",
      " 'country_code_bin_3' 'country_code_bin_4' 'country_code_bin_5'\n",
      " 'country_code_bin_6' 'country_code_bin_7' 'country_code_bin_8'\n",
      " 'day_of_week_bin_0' 'day_of_week_bin_1' 'day_of_week_bin_2'\n",
      " 'day_of_week_bin_3' 'day_of_week_sin' 'day_of_week_cos'\n",
      " 'day_of_month_sin' 'day_of_month_cos' 'day_of_year_sin' 'day_of_year_cos'\n",
      " 'date_day']\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train_prescaled.copy()\n",
    "df_validation = df_validation_prescaled.copy()\n",
    "df_test = df_test_prescaled.copy()\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "print(df_ml.columns.values)\n",
    "for feature_name in df_ml.columns.values:\n",
    "    if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "        continue\n",
    "    scalers[feature_name] = StandardScaler()\n",
    "    df_train[feature_name] = scalers[feature_name].fit_transform(df_train_prescaled[[feature_name]])\n",
    "        \n",
    "if len(df_validation_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue\n",
    "        df_validation[feature_name] = scalers[feature_name].transform(df_validation_prescaled[[feature_name]])\n",
    "\n",
    "if len(df_test_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'geo_code' or feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue        \n",
    "        df_test[feature_name] = scalers[feature_name].transform(df_test_prescaled[[feature_name]])\n",
    "\n",
    "df_train = df_train.drop(labels=['geo_code', 'date'], axis=1)\n",
    "df_validation = df_validation.drop(labels=['geo_code', 'date'], axis=1)\n",
    "df_test = df_test.drop(labels=['geo_code', 'date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:8091.68994\ttrain-rmse:4131.47510\n",
      "[10]\teval-rmse:4195.99463\ttrain-rmse:3324.67139\n",
      "[20]\teval-rmse:4029.14404\ttrain-rmse:3270.61499\n",
      "[30]\teval-rmse:4028.06201\ttrain-rmse:3254.31934\n",
      "[40]\teval-rmse:4027.02441\ttrain-rmse:3247.43579\n",
      "[50]\teval-rmse:4021.57471\ttrain-rmse:3243.72852\n",
      "[60]\teval-rmse:4013.83936\ttrain-rmse:3241.34839\n",
      "[70]\teval-rmse:4005.06689\ttrain-rmse:3239.60864\n",
      "[80]\teval-rmse:3995.99072\ttrain-rmse:3238.23291\n",
      "[90]\teval-rmse:3987.03906\ttrain-rmse:3237.09521\n",
      "[99]\teval-rmse:3979.32056\ttrain-rmse:3236.22192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "params_tree = {\n",
    "    'nthread': 1,\n",
    "    'objective':'reg:squarederror',\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "\n",
    "params_linear = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "\n",
    "\n",
    "rounds = 100\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "test_x, test_y = df_test.iloc[:, 1:], df_test.iloc[:, :1]\n",
    "dtest = xgb.DMatrix(data=test_x,label=test_y)\n",
    "callback_monitor = xgb.callback.EvaluationMonitor(rank=0, period=10, show_stdv=False)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=df_train.iloc[:, 1:], label=df_train.iloc[:, :1])\n",
    "dvalidation = xgb.DMatrix(data=df_validation.iloc[:, 1:], label=df_validation.iloc[:, :1])\n",
    "watchlist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(params_linear, \n",
    "                dtrain, \n",
    "                rounds,\n",
    "                watchlist,\n",
    "                early_stopping_rounds=20,      \n",
    "                callbacks=[callback_monitor],\n",
    "                verbose_eval=False)\n",
    "bst.save_model('temp/predictor.model')\n",
    "predictions = bst.predict(dtest)\n",
    "score = mean_squared_error(test_y, predictions, squared=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the Prescription Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-11 16:17:09.500686\n",
      "generating predictions for index 0\n",
      "assigning npi values\n",
      "scaling\n",
      "predicting\n",
      "            date     new_cases  confirmed_cases  c1_school_closing\n",
      "25606 2021-02-12  1.828189e+09     1.830486e+09                3.0\n",
      "25607 2021-02-13  1.819276e+09     1.821572e+09                3.0\n",
      "25608 2021-02-14  1.811876e+09     1.814173e+09                3.0\n",
      "25609 2021-02-15  1.805673e+09     1.807969e+09                3.0\n",
      "25610 2021-02-16  1.800749e+09     1.803046e+09                3.0\n",
      "25611 2021-02-17  1.797499e+09     1.799796e+09                3.0\n",
      "25612 2021-02-18  1.793725e+09     1.796021e+09                3.0\n",
      "2021-02-11 16:29:17.364573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_ma(df_x, field, window_size):\n",
    "    df_x[f\"{field}_ma_{window_size}\"] = df_x.groupby('geo_code')[field].rolling(window_size, center=False).mean().fillna(0).reset_index(0, drop=True)\n",
    "\n",
    "    \n",
    "def predict(prescription):\n",
    "    # filter the date range for the prediction window\n",
    "    days_to_predict = 10\n",
    "    date_to_predict_from = df[df['predicted'] == False]['date'].max()\n",
    "    date_to_predict_to = date_to_predict_from + pd.to_timedelta(days_to_predict, unit='d')  \n",
    "    df_out = df.copy()\n",
    "    df_out = df_out.sort_values(['geo_code', 'date'])\n",
    "    df_out = df_out[df_out['date'] <= pd.to_datetime(date_to_predict_to)]\n",
    "    df_out['region_name'] = df_out['region_name'].fillna('')\n",
    "    \n",
    "    \n",
    "    # assign npi values\n",
    "    print(\"assigning npi values\")\n",
    "    for _, row in df_out.loc[df_out['date'] >= date_to_predict_from].iterrows():\n",
    "        date_from = row['date']\n",
    "        country_name = row['country_name']\n",
    "        region_name = row['region_name']\n",
    "        filter = ((prescription['Date'] == date_from) & (prescription['CountryName'] == country_name) & (prescription['RegionName'] == region_name))\n",
    "        prescription_row = prescription.loc[filter] \n",
    "        row['c1_school_closing'] = prescription_row['C1_School closing'].max()\n",
    "        row['c2_workplace_closing'] = prescription_row['C2_Workplace closing'].max()\n",
    "        row['c3_cancel_public_events'] = prescription_row['C3_Cancel public events'].max()\n",
    "        row['c4_restrictions_on_gatherings'] = prescription_row['C4_Restrictions on gatherings'].max()\n",
    "        row['c5_close_public_transport'] = prescription_row['C5_Close public transport'].max()\n",
    "        row['c6_stay_at_home_requirements'] = prescription_row['C6_Stay at home requirements'].max()\n",
    "        row['c7_restrictions_on_internal_movement'] = prescription_row['C7_Restrictions on internal movement'].max()\n",
    "        row['c8_international_travel_controls'] = prescription_row['C8_International travel controls'].max()\n",
    "        row['h1_public_information_campaigns'] = prescription_row['H1_Public information campaigns'].max()\n",
    "        row['h2_testing_policy'] = prescription_row['H2_Testing policy'].max()\n",
    "        row['h3_contact_tracing'] = prescription_row['H3_Contact tracing'].max()\n",
    "        row['h6_facial_coverings'] = prescription_row['H6_Facial Coverings'].max()            \n",
    "    \n",
    "\n",
    "    # encode the data\n",
    "    df_input = encode(df_out.copy(), fit=False)\n",
    "\n",
    "     # get iterators we'll use on the rows\n",
    "    it_df = df_out.loc[df_out['date'] >= date_to_predict_from].iterrows()\n",
    "    it_ml = df_input.loc[df_input['date'] >= date_to_predict_from].iterrows()\n",
    "    \n",
    "    # scale values\n",
    "    print(\"scaling\")\n",
    "    for name in scalers:\n",
    "        if name not in df_input.columns:\n",
    "            continue\n",
    "        if name == 'confirmed_cases':\n",
    "            continue\n",
    "        if name == 'new_cases':\n",
    "            continue\n",
    "        df_input[name] = scalers[name].transform(df_input[[name]])\n",
    "    \n",
    "    \n",
    "    # predict each day\n",
    "    print(\"predicting\")\n",
    "    for _, row in it_df:\n",
    "        geo_code = row['geo_code']\n",
    "        confirmed_cases = row['confirmed_cases']\n",
    "        date_from = row['date']\n",
    "        date_to = date_from + pd.to_timedelta(1, unit='d')  \n",
    "                \n",
    "        # conver to df\n",
    "        model_input = next(it_ml)[1]    \n",
    "        model_input = pd.DataFrame(model_input.to_frame().T)\n",
    "        model_input = model_input.drop(labels=['geo_code', 'date'], axis=1)\n",
    "        for name in model_input.columns:\n",
    "            model_input[name] = pd.to_numeric(model_input[name])\n",
    "        model_input['new_cases'] = scalers['new_cases'].transform(model_input[['new_cases']])        \n",
    "        model_input['confirmed_cases'] = scalers['confirmed_cases'].transform(model_input[['confirmed_cases']])\n",
    "                \n",
    "        # predict\n",
    "        predictions = bst.predict(xgb.DMatrix(data=model_input.iloc[:, 1:], label=model_input.iloc[:, :1]))\n",
    "        new_cases = max(0., predictions[0])\n",
    "        \n",
    "        # assign new cases and predicted cases to next row\n",
    "        filter_out = (df_out['geo_code'] == geo_code) & (df_out['date'] == date_to)\n",
    "        filter_in = (df_input['geo_code'] == geo_code) & (df_input['date'] == date_to)        \n",
    "        df_out.loc[filter_out, ['new_cases', 'confirmed_cases']] = [new_cases, confirmed_cases + new_cases]\n",
    "        df_input.loc[filter_in, ['new_cases', 'confirmed_cases']] = [new_cases, confirmed_cases + new_cases]\n",
    "        \n",
    "        # update the moving averages for new and confirmed cases\n",
    "        for window_size in [3, 7, 21]:\n",
    "            compute_ma(df_input, 'new_cases', window_size)\n",
    "            compute_ma(df_input, 'confirmed_cases', window_size)\n",
    "\n",
    "    # debug    \n",
    "    print(df_out.loc[df_out['geo_code'] == 'DE'][['date', 'new_cases', 'confirmed_cases', 'c1_school_closing']].tail(7))\n",
    "        \n",
    "        \n",
    "\n",
    "# read the prescriptions and generate a prediction file for each prescription\n",
    "def do_predictions():\n",
    "    prescriptions = pd.read_csv('prescriptions.csv', keep_default_na=False, na_values='')\n",
    "    prescriptions['Date'] = pd.to_datetime(prescriptions['Date']) \n",
    "    for i in range(1):\n",
    "        print(f\"generating predictions for index {i}\")\n",
    "        predict(prescriptions.loc[prescriptions['PrescriptionIndex'] == i])\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "do_predictions()\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a1823d221b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geo_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_cases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confirmed_cases'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_predict' is not defined"
     ]
    }
   ],
   "source": [
    "df_predict.loc[df_predict['geo_code'] == 'DE'][['date', 'new_cases', 'confirmed_cases']].tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
