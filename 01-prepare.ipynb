{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall git+https://github.com/rbilleci/pandora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pandora\n",
    "import pandora.data.age_distribution as age_dist\n",
    "import pandora.data.oxford_data as oxford\n",
    "import pandora.data.population as population\n",
    "import pandora.data.temperatures as temperatures\n",
    "import shutil\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from pandora.data import geo, continent, country_code, working_day\n",
    "from pandora import loader, encoders\n",
    "from pandora.core_fields import DATE, COUNTRY_CODE\n",
    "from sagemaker import get_execution_role, session\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from elasticsearch import helpers\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandora.data.oxford_data_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# determine the last date we have from the oxford data set\n",
    "# which is the first day we'll begin prediction from\n",
    "prediction_start_date = pd.read_csv(oxford.module.location, keep_default_na=False, na_values='')['date'].max()\n",
    "prediction_start_date = datetime.datetime.strptime(prediction_start_date, '%Y-%m-%d').date()\n",
    "\n",
    "# the data range should cover the ground truth data + the time window we are predicting into\n",
    "days_to_predict = 90\n",
    "start_date = datetime.date(2020, 1, 1)\n",
    "end_date = prediction_start_date + datetime.timedelta(days=days_to_predict)\n",
    "\n",
    "# the imputation window might extend far before or after\n",
    "# the range we are predicting, this is so we have more data samples for imputation calculations\n",
    "imputation_window_start_date =  datetime.date(2020, 1, 1)\n",
    "imputation_window_end_date =  datetime.date(2021, 12, 31)\n",
    "\n",
    "# load the data\n",
    "df = loader.load(start_date,\n",
    "                 end_date,\n",
    "                 imputation_window_start_date,\n",
    "                 imputation_window_end_date,\n",
    "                 geo.module,\n",
    "                 [\n",
    "                     country_code.module,\n",
    "                     continent.module,\n",
    "                     population.module,\n",
    "                     age_dist.module,\n",
    "                     temperatures.module,\n",
    "                     oxford.module,\n",
    "                     working_day.module\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add computed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ma(field, window_size):\n",
    "    df[f\"{field}_ma_{window_size}\"] = df.groupby('geo_code')[field].rolling(window_size, center=False).mean().fillna(0).reset_index(0, drop=True)\n",
    "\n",
    "def add_working_day_tomorrow(grouped):\n",
    "    grouped['working_day' + '_tomorrow'] = grouped['working_day'].copy().shift(-1).bfill().ffill()\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def add_working_day_yesterday(grouped):\n",
    "    grouped['working_day' + '_yesterday'] = grouped['working_day'].copy().shift(1).bfill().ffill()\n",
    "    return grouped\n",
    "\n",
    "def transform_column_order(df):\n",
    "    df = df.reindex(sorted(df.columns), axis=1)  # Sort columns by name\n",
    "    df_label = df['predicted_new_cases']\n",
    "    df = df.drop(labels=['predicted_new_cases'], axis=1)\n",
    "    df.insert(0, 'predicted_new_cases', df_label)\n",
    "    return df\n",
    "\n",
    "# Compute number of new cases and deaths each day\n",
    "# Replace negative values (which do not make sense for these columns) with 0\n",
    "df['new_cases'] = df.groupby('geo_code').confirmed_cases.diff().fillna(0)\n",
    "df['new_cases'] = df['new_cases'].clip(lower=0)\n",
    "\n",
    "# add predicted new cases\n",
    "df['predicted_new_cases'] = df.groupby('geo_code').new_cases.shift(-1).fillna(0)\n",
    "df['predicted_new_cases'] = df['predicted_new_cases'].clip(lower=0)\n",
    "\n",
    "# add confirmed cases as percent of population\n",
    "df['new_cases_as_percent_of_population'] = df['new_cases'] / df['population']\n",
    "df['confirmed_cases_as_percent_of_population'] = df['confirmed_cases'] / df['population']\n",
    "\n",
    "# Add moving averages\n",
    "for window_size in [3, 7, 21]:\n",
    "    compute_ma('new_cases', window_size)\n",
    "    compute_ma('confirmed_cases', window_size)\n",
    "    compute_ma('specific_humidity', window_size)    \n",
    "    compute_ma('temperature', window_size)        \n",
    "    compute_ma('c1_school_closing', window_size)        \n",
    "    compute_ma('c2_workplace_closing', window_size)        \n",
    "    compute_ma('c3_cancel_public_events', window_size)        \n",
    "    compute_ma('c4_restrictions_on_gatherings', window_size)   \n",
    "    compute_ma('c5_close_public_transport', window_size)        \n",
    "    compute_ma('c6_stay_at_home_requirements', window_size)        \n",
    "    compute_ma('c7_restrictions_on_internal_movement', window_size)        \n",
    "    compute_ma('c8_international_travel_controls', window_size)   \n",
    "    compute_ma('h1_public_information_campaigns', window_size)        \n",
    "    compute_ma('h2_testing_policy', window_size)        \n",
    "    compute_ma('h3_contact_tracing', window_size)        \n",
    "    compute_ma('h6_facial_coverings', window_size)     \n",
    "    compute_ma('working_day', window_size)        \n",
    "    compute_ma('new_cases_as_percent_of_population', window_size)     \n",
    "    compute_ma('confirmed_cases_as_percent_of_population', window_size)     \n",
    "\n",
    "# Add working day information for tomorrow, and today\n",
    "df = df.groupby('geo_code').apply(lambda group: add_working_day_tomorrow(group)).reset_index(drop=True)\n",
    "df = df.groupby('geo_code').apply(lambda group: add_working_day_yesterday(group)).reset_index(drop=True)\n",
    "df['npi_sum'] = df['c1_school_closing'] + df['c2_workplace_closing'] + \\\n",
    "                df['c3_cancel_public_events'] + df['c4_restrictions_on_gatherings'] + \\\n",
    "                df['c5_close_public_transport'] + df['c6_stay_at_home_requirements'] + \\\n",
    "                df['c7_restrictions_on_internal_movement'] + df['c8_international_travel_controls'] + \\\n",
    "                df['h1_public_information_campaigns'] + df['h2_testing_policy'] + \\\n",
    "                df['h3_contact_tracing'] + df['h6_facial_coverings']\n",
    "\n",
    "# Drop unused columns\n",
    "df = transform_column_order(df)\n",
    "df = df.sort_values(['geo_code', 'date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('temp', ignore_errors=True)\n",
    "Path('temp').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "df.to_csv('temp/01-data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
