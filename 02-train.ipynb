{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Collecting git+https://github.com/rbilleci/pandora.git\n",
      "  Cloning https://github.com/rbilleci/pandora.git to /tmp/pip-req-build-a7w_ctbc\n",
      "  Running command git clone -q https://github.com/rbilleci/pandora.git /tmp/pip-req-build-a7w_ctbc\n",
      "Requirement already satisfied (use --upgrade to upgrade): pandora==0.1.0 from git+https://github.com/rbilleci/pandora.git in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: pandas~=1.2.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: fnvhash~=0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn~=0.24.1 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (0.24.1)\n",
      "Requirement already satisfied: workalendar~=14.1.0 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: category-encoders~=2.2.2 in /opt/conda/lib/python3.7/site-packages (from pandora==0.1.0) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas~=1.2.1->pandora==0.1.0) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn~=0.24.1->pandora==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: skyfield-data in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pyluach in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: skyfield in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.36)\n",
      "Requirement already satisfied: lunardate in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: pyCalverter in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: setuptools>=1.0 in /opt/conda/lib/python3.7/site-packages (from workalendar~=14.1.0->pandora==0.1.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category-encoders~=2.2.2->pandora==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas~=1.2.1->pandora==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: jplephem>=2.13 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2019.11.28)\n",
      "Requirement already satisfied: sgp4>=2.2 in /opt/conda/lib/python3.7/site-packages (from skyfield->workalendar~=14.1.0->pandora==0.1.0) (2.15)\n",
      "Building wheels for collected packages: pandora\n",
      "  Building wheel for pandora (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandora: filename=pandora-0.1.0-py3-none-any.whl size=2681412 sha256=c3747069c2561901e403d263153eb5ed96c5ab86d6f89b30d892701e81850a19\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2gablfqc/wheels/01/8b/d5/a72c927a738750e04a4bb4fd22f63b4b88c7b5871732e2d67b\n",
      "Successfully built pandora\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install git+https://github.com/rbilleci/pandora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from pandora import loader, encoders\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (98) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and set the date column\n",
    "df = pd.read_csv('temp/01-data.csv', keep_default_na=False, na_values='')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# determine the date where prediction should begin\n",
    "prediction_start_date = df[df['predicted'] == True]['date'].min().date()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# only work within the specified range\n",
    "df_ml = df.loc[df['date'] < pd.to_datetime(prediction_start_date)]\n",
    "enc = {}\n",
    "enc['continent'] = encoders.BinaryEncoder('continent')\n",
    "enc['geo_code'] = encoders.BinaryEncoder('geo_code')\n",
    "enc['country_code'] = encoders.BinaryEncoder('country_code')\n",
    "enc['day_of_week'] = encoders.OneHotEncoder('day_of_week')\n",
    "enc['day_of_week_cyc'] = encoders.CyclicalEncoder('day_of_week')\n",
    "enc['day_of_month'] = encoders.OneHotEncoder('day_of_month')\n",
    "enc['day_of_month_cyc'] = encoders.CyclicalEncoder('day_of_month')\n",
    "enc['day_of_year'] = encoders.BinaryEncoder('day_of_year')\n",
    "enc['day_of_year_cyc'] = encoders.CyclicalEncoder('day_of_year')\n",
    "\n",
    "def encode(df_x, fit):\n",
    "    # convert the date to an integer value\n",
    "    df_x['date_day'] = df_x['date'].apply(lambda x: x.day)\n",
    "\n",
    "    # encode the geo data\n",
    "    if fit:\n",
    "        df_x = enc['continent'].fit_transform(df_x)\n",
    "        df_x = enc['geo_code'].fit_transform(df_x)\n",
    "        df_x = enc['country_code'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['continent'].transform(df_x)\n",
    "        df_x = enc['geo_code'].transform(df_x)\n",
    "        df_x = enc['country_code'].transform(df_x)\n",
    "    if fit:\n",
    "        df_x = enc['day_of_week'].fit_transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'] / 7.0\n",
    "        df_x = enc['day_of_week_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['day_of_week'].transform(df_x)\n",
    "        df_x['day_of_week'] = df_x['day_of_week'] / 7.0\n",
    "        df_x = enc['day_of_week_cyc'].transform(df_x)\n",
    "    if fit:\n",
    "        df_x = enc['day_of_month'].fit_transform(df_x)\n",
    "        df_x['day_of_month'] = df_x['day_of_month'] / 31.0 # keep it simple\n",
    "        df_x = enc['day_of_month_cyc'].fit_transform(df_x)\n",
    "        df_x = enc['day_of_year'].fit_transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'] / 366.0 # keep it simple\n",
    "        df_x = enc['day_of_year_cyc'].fit_transform(df_x)\n",
    "    else:\n",
    "        df_x = enc['day_of_month'].transform(df_x)\n",
    "        df_x['day_of_month'] = df_x['day_of_month'] / 31.0 # keep it simple\n",
    "        df_x = enc['day_of_month_cyc'].transform(df_x)\n",
    "        df_x = enc['day_of_year'].transform(df_x)\n",
    "        df_x['day_of_year'] = df_x['day_of_year'] / 366.0 # keep it simple\n",
    "        df_x = enc['day_of_year_cyc'].transform(df_x)\n",
    "        \n",
    "    \n",
    "    # drop unused columns\n",
    "    df_x = df_x.drop(labels=['country_name',\n",
    "                           'continent',\n",
    "                           'geo_code',\n",
    "                           'country_code',\n",
    "                           'day_of_week',\n",
    "                           'day_of_month',\n",
    "                           'day_of_year',\n",
    "                            'country_code3',\n",
    "                            'country_code_numeric',\n",
    "                            'confirmed_deaths',\n",
    "                            'predicted',\n",
    "                            'region_name',\n",
    "                            'month',                           \n",
    "                            'quarter',\n",
    "                            'week'], axis=1)\n",
    "    return df_x\n",
    "\n",
    "df_ml = encode(df_ml, fit=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train, val, test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Range:   2020-01-01 - 2020-12-25\n",
      "Validation Range: 2020-12-26 - 2021-01-25\n",
      "Test Range:       2021-01-26 - 2021-02-08\n"
     ]
    }
   ],
   "source": [
    "days_for_validation = 31\n",
    "days_for_test = 14\n",
    "\n",
    "def split(df: pd.DataFrame, \n",
    "          days_for_validation: int, \n",
    "          days_for_test: int) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # First, sort the data by date\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Determine the maximum date\n",
    "    date_start_test = df['date'].max() - pd.to_timedelta(days_for_test - 1, unit='d')\n",
    "    date_start_validation = date_start_test - pd.to_timedelta(days_for_validation, unit='d')\n",
    "\n",
    "    df_train = df[df['date'] < date_start_validation]\n",
    "    df_validation = df[(df['date'] >= date_start_validation) & (df['date'] < date_start_test)]\n",
    "    df_test = df[df['date'] >= date_start_test]\n",
    "\n",
    "    # Debug the outpoint\n",
    "    print(f\"Training Range:   {df_train['date'].min().date()} - {df_train['date'].max().date()}\")\n",
    "    print(f\"Validation Range: {df_validation['date'].min().date()} - {df_validation['date'].max().date()}\")\n",
    "    print(f\"Test Range:       {df_test['date'].min().date()} - {df_test['date'].max().date()}\")\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(df.index) != len(df_train.index) + len(df_validation.index) + len(df_test.index):\n",
    "        raise Exception('entries do not add up')\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "df_train_prescaled, df_validation_prescaled, df_test_prescaled = split(df_ml, days_for_validation, days_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_prescaled.copy()\n",
    "df_validation = df_validation_prescaled.copy()\n",
    "df_test = df_test_prescaled.copy()\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "for feature_name in df_ml.columns.values:\n",
    "    if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "        continue\n",
    "    scalers[feature_name] = StandardScaler()\n",
    "    df_train[feature_name] = scalers[feature_name].fit_transform(df_train_prescaled[[feature_name]])\n",
    "        \n",
    "if len(df_validation_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue\n",
    "        df_validation[feature_name] = scalers[feature_name].transform(df_validation_prescaled[[feature_name]])\n",
    "\n",
    "if len(df_test_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue        \n",
    "        df_test[feature_name] = scalers[feature_name].transform(df_test_prescaled[[feature_name]])\n",
    "\n",
    "df_train = df_train.drop(labels=['date'], axis=1)\n",
    "df_validation = df_validation.drop(labels=['date'], axis=1)\n",
    "df_test = df_test.drop(labels=['date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO Baby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_train.to_csv(f\"hpo_train.csv\", index=False, header=False)\\ndf_validation.to_csv(f\"hpo_validation.csv\", index=False, header=False)\\ndf_test.to_csv(f\"hpo_test.csv\", index=False, header=False)\\n\\n# push to s3\\nboto3.Session().resource(\\'s3\\').Bucket(bucket).Object(os.path.join(\\'hpo\\', \\'train\\')).upload_file(\\'hpo_train.csv\\')\\nboto3.Session().resource(\\'s3\\').Bucket(bucket).Object(os.path.join(\\'hpo\\', \\'validation\\')).upload_file(\\'hpo_validation.csv\\')\\nboto3.Session().resource(\\'s3\\').Bucket(bucket).Object(os.path.join(\\'hpo\\', \\'test\\')).upload_file(\\'hpo_test.csv\\')\\n'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# this is a manual step to determine the best parameters...\n",
    "\n",
    "# note that HPO does not have a header column\n",
    "\"\"\"\n",
    "df_train.to_csv(f\"hpo_train.csv\", index=False, header=False)\n",
    "df_validation.to_csv(f\"hpo_validation.csv\", index=False, header=False)\n",
    "df_test.to_csv(f\"hpo_test.csv\", index=False, header=False)\n",
    "\n",
    "# push to s3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('hpo', 'train')).upload_file('hpo_train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('hpo', 'validation')).upload_file('hpo_validation.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('hpo', 'test')).upload_file('hpo_test.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "params_hpo = {\n",
    "    'alpha': 0.03244945870956029,\n",
    "    'nthread': 1,\n",
    "    'objective':'reg:squarederror',\n",
    "    'eta': 0.16740916905083,\n",
    "    'colsample_bylevel': 0.8953507439536397,\n",
    "    'colsample_bytree': 0.9459754246923018,\n",
    "    'gamma': 0.4185824423681137,\n",
    "    'max_delta_step': 0,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 0.8210632572458949,\n",
    "    'subsample': 0.7371710015275215\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "params_tree = {\n",
    "    'nthread': 1,\n",
    "    'objective':'reg:squarederror',\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "params_dart = {\n",
    "    \"booster\": \"dart\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"rate_drop\": 0.1\n",
    "}\n",
    "\n",
    "params_linear = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    'nthread': 1,    \n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:8100.29346\ttrain-rmse:4129.94922\n",
      "[10]\teval-rmse:4225.42432\ttrain-rmse:3324.32739\n",
      "[20]\teval-rmse:4043.60181\ttrain-rmse:3270.14624\n",
      "[30]\teval-rmse:4039.64380\ttrain-rmse:3253.82935\n",
      "[40]\teval-rmse:4037.38037\ttrain-rmse:3246.95752\n",
      "[50]\teval-rmse:4030.91553\ttrain-rmse:3243.26343\n",
      "[60]\teval-rmse:4022.33081\ttrain-rmse:3240.89331\n",
      "[70]\teval-rmse:4012.86548\ttrain-rmse:3239.16211\n",
      "[80]\teval-rmse:4003.22412\ttrain-rmse:3237.78980\n",
      "[90]\teval-rmse:3993.83057\ttrain-rmse:3236.65527\n",
      "[100]\teval-rmse:3984.92651\ttrain-rmse:3235.69653\n",
      "[110]\teval-rmse:3976.62744\ttrain-rmse:3234.87793\n",
      "[120]\teval-rmse:3968.97095\ttrain-rmse:3234.18115\n",
      "[130]\teval-rmse:3961.95581\ttrain-rmse:3233.57764\n",
      "[140]\teval-rmse:3955.55493\ttrain-rmse:3233.06323\n",
      "[150]\teval-rmse:3949.72559\ttrain-rmse:3232.61621\n",
      "[160]\teval-rmse:3944.42090\ttrain-rmse:3232.23022\n",
      "[170]\teval-rmse:3939.58520\ttrain-rmse:3231.89893\n",
      "[180]\teval-rmse:3935.17749\ttrain-rmse:3231.60644\n",
      "[190]\teval-rmse:3931.15210\ttrain-rmse:3231.35889\n",
      "[200]\teval-rmse:3927.47754\ttrain-rmse:3231.14356\n",
      "[210]\teval-rmse:3924.11035\ttrain-rmse:3230.95483\n",
      "[220]\teval-rmse:3921.02368\ttrain-rmse:3230.79004\n",
      "[230]\teval-rmse:3918.18872\ttrain-rmse:3230.64526\n",
      "[240]\teval-rmse:3915.58374\ttrain-rmse:3230.51685\n",
      "[250]\teval-rmse:3913.18433\ttrain-rmse:3230.40601\n",
      "[260]\teval-rmse:3910.97070\ttrain-rmse:3230.30518\n",
      "[270]\teval-rmse:3908.92554\ttrain-rmse:3230.21655\n",
      "[280]\teval-rmse:3907.03516\ttrain-rmse:3230.13769\n",
      "[290]\teval-rmse:3905.28491\ttrain-rmse:3230.06958\n",
      "[300]\teval-rmse:3903.66260\ttrain-rmse:3230.00610\n",
      "[310]\teval-rmse:3902.16113\ttrain-rmse:3229.94946\n",
      "[320]\teval-rmse:3900.76758\ttrain-rmse:3229.89844\n",
      "[330]\teval-rmse:3899.47437\ttrain-rmse:3229.85254\n",
      "[340]\teval-rmse:3898.27417\ttrain-rmse:3229.81055\n",
      "[350]\teval-rmse:3897.15308\ttrain-rmse:3229.77417\n",
      "[360]\teval-rmse:3896.10962\ttrain-rmse:3229.73975\n",
      "[370]\teval-rmse:3895.13623\ttrain-rmse:3229.70923\n",
      "[380]\teval-rmse:3894.22827\ttrain-rmse:3229.68091\n",
      "[390]\teval-rmse:3893.38379\ttrain-rmse:3229.65283\n",
      "[400]\teval-rmse:3892.59790\ttrain-rmse:3229.63013\n",
      "[410]\teval-rmse:3891.86011\ttrain-rmse:3229.60889\n",
      "[420]\teval-rmse:3891.16626\ttrain-rmse:3229.58911\n",
      "[430]\teval-rmse:3890.52441\ttrain-rmse:3229.57007\n",
      "[440]\teval-rmse:3889.92114\ttrain-rmse:3229.55322\n",
      "[450]\teval-rmse:3889.35498\ttrain-rmse:3229.53564\n",
      "[460]\teval-rmse:3888.83008\ttrain-rmse:3229.52051\n",
      "[470]\teval-rmse:3888.33008\ttrain-rmse:3229.50684\n",
      "[480]\teval-rmse:3887.86377\ttrain-rmse:3229.49536\n",
      "[490]\teval-rmse:3887.42651\ttrain-rmse:3229.48144\n",
      "[500]\teval-rmse:3887.02075\ttrain-rmse:3229.47144\n",
      "[510]\teval-rmse:3886.63306\ttrain-rmse:3229.46118\n",
      "[520]\teval-rmse:3886.26904\ttrain-rmse:3229.45142\n",
      "[530]\teval-rmse:3885.92993\ttrain-rmse:3229.44189\n",
      "[540]\teval-rmse:3885.61035\ttrain-rmse:3229.43530\n",
      "[550]\teval-rmse:3885.31104\ttrain-rmse:3229.42700\n",
      "[560]\teval-rmse:3885.02686\ttrain-rmse:3229.41919\n",
      "[570]\teval-rmse:3884.75732\ttrain-rmse:3229.41187\n",
      "[580]\teval-rmse:3884.50464\ttrain-rmse:3229.40601\n",
      "[590]\teval-rmse:3884.26856\ttrain-rmse:3229.39893\n",
      "[600]\teval-rmse:3884.04590\ttrain-rmse:3229.39209\n",
      "[610]\teval-rmse:3883.83618\ttrain-rmse:3229.38574\n",
      "[620]\teval-rmse:3883.63892\ttrain-rmse:3229.38135\n",
      "[630]\teval-rmse:3883.44580\ttrain-rmse:3229.37573\n",
      "[640]\teval-rmse:3883.26367\ttrain-rmse:3229.37183\n",
      "[650]\teval-rmse:3883.09668\ttrain-rmse:3229.36572\n",
      "[660]\teval-rmse:3882.93945\ttrain-rmse:3229.36206\n",
      "[670]\teval-rmse:3882.78638\ttrain-rmse:3229.35864\n",
      "[680]\teval-rmse:3882.64331\ttrain-rmse:3229.35474\n",
      "[690]\teval-rmse:3882.50903\ttrain-rmse:3229.35156\n",
      "[700]\teval-rmse:3882.38623\ttrain-rmse:3229.34741\n",
      "[710]\teval-rmse:3882.26489\ttrain-rmse:3229.34326\n",
      "[720]\teval-rmse:3882.15015\ttrain-rmse:3229.34009\n",
      "[730]\teval-rmse:3882.04150\ttrain-rmse:3229.33594\n",
      "[740]\teval-rmse:3881.93823\ttrain-rmse:3229.33301\n",
      "[750]\teval-rmse:3881.84009\ttrain-rmse:3229.32910\n",
      "[760]\teval-rmse:3881.74731\ttrain-rmse:3229.32617\n",
      "[770]\teval-rmse:3881.65991\ttrain-rmse:3229.32324\n",
      "[780]\teval-rmse:3881.57471\ttrain-rmse:3229.31958\n",
      "[790]\teval-rmse:3881.49487\ttrain-rmse:3229.31812\n",
      "[800]\teval-rmse:3881.41992\ttrain-rmse:3229.31543\n",
      "[810]\teval-rmse:3881.34985\ttrain-rmse:3229.31226\n",
      "[820]\teval-rmse:3881.28027\ttrain-rmse:3229.31030\n",
      "[830]\teval-rmse:3881.21582\ttrain-rmse:3229.30933\n",
      "[840]\teval-rmse:3881.15527\ttrain-rmse:3229.30713\n",
      "[850]\teval-rmse:3881.09790\ttrain-rmse:3229.30444\n",
      "[860]\teval-rmse:3881.04126\ttrain-rmse:3229.30249\n",
      "[870]\teval-rmse:3880.99048\ttrain-rmse:3229.30029\n",
      "[880]\teval-rmse:3880.94287\ttrain-rmse:3229.29907\n",
      "[890]\teval-rmse:3880.89160\ttrain-rmse:3229.29834\n",
      "[900]\teval-rmse:3880.84814\ttrain-rmse:3229.29517\n",
      "[910]\teval-rmse:3880.80273\ttrain-rmse:3229.29517\n",
      "[914]\teval-rmse:3880.78540\ttrain-rmse:3229.29492\n",
      "4297.474437630029\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "use_walk_forward = False\n",
    "\n",
    "test_x, test_y = df_test.iloc[:, 1:], df_test.iloc[:, :1]\n",
    "dtest = xgb.DMatrix(data=test_x,label=test_y)\n",
    "early_stopping_rounds = 10\n",
    "callback_monitor = xgb.callback.EvaluationMonitor(rank=0, period=10, show_stdv=False)\n",
    "\n",
    "if use_walk_forward:\n",
    "    df_train_and_validation = pd.concat([df_train, df_validation])\n",
    "    training_loops = 3\n",
    "    training_steps = 50\n",
    "    records = df_train_and_validation.shape[0]\n",
    "    records_per_step = int(records / training_steps)\n",
    "\n",
    "\n",
    "    # The number of training rounds\n",
    "    trained_once = False\n",
    "    for z in range(0, training_loops):\n",
    "\n",
    "        # Step through the dataset\n",
    "        for step in range(0, training_steps):\n",
    "\n",
    "            # get the ranges we'll work from\n",
    "            train_start = records_per_step * step\n",
    "            val_start = records_per_step * (step + 1)\n",
    "            val_end = val_start + records_per_step\n",
    "            \n",
    "            # make sure there is sufficient validation data available to run an iteration\n",
    "            if (records - val_start) < records_per_step:\n",
    "                print(f\"finishing walk with {records - val_start} records remaining\")\n",
    "                break\n",
    "\n",
    "            # slice our training and validation sets\n",
    "            train, val = df_train_and_validation[train_start:val_start], df_train_and_validation[val_start:val_end]\n",
    "\n",
    "            # Get the train and val data, then train the model\n",
    "            tx, ty = train.iloc[:, 1:], train.iloc[:, :1]\n",
    "            vx, vy = val.iloc[:, 1:], val.iloc[:, :1]\n",
    "\n",
    "            # setup the training dataset        \n",
    "            dtrain = xgb.DMatrix(data=tx,label=ty)\n",
    "            dvalidation = xgb.DMatrix(data=vx,label=vy)\n",
    "            watchlist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "            if trained_once:\n",
    "                bst = xgb.train(params_linear, \n",
    "                                dtrain, \n",
    "                                1000, \n",
    "                                watchlist, \n",
    "                                early_stopping_rounds=early_stopping_rounds,\n",
    "                                verbose_eval=False,\n",
    "                                xgb_model='temp/predictor.model')\n",
    "                bst.save_model('temp/predictor.model')\n",
    "            else:\n",
    "                bst = xgb.train(params_linear, \n",
    "                                dtrain, \n",
    "                                1000, \n",
    "                                watchlist,\n",
    "                                early_stopping_rounds=early_stopping_rounds,      \n",
    "                                verbose_eval=False)\n",
    "                bst.save_model('temp/predictor.model')\n",
    "                trained_once = True\n",
    "\n",
    "\n",
    "            # get the status\n",
    "            predictions = bst.predict(dtest)\n",
    "            score = mean_squared_error(test_y, predictions, squared=False)\n",
    "            print(f\"walk={z+1}/{training_loops}, step={step+1}/{training_steps}, {len(train)}, {len(val)}, rmse={score}\")\n",
    "else:\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=df_train.iloc[:, 1:], label=df_train.iloc[:, :1])\n",
    "    dvalidation = xgb.DMatrix(data=df_validation.iloc[:, 1:], label=df_validation.iloc[:, :1])\n",
    "    watchlist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "    \n",
    "    bst = xgb.train(params_linear, \n",
    "                    dtrain, \n",
    "                    1000, \n",
    "                    watchlist,\n",
    "                    early_stopping_rounds=early_stopping_rounds,      \n",
    "                    callbacks=[callback_monitor],\n",
    "                    verbose_eval=False)\n",
    "    bst.save_model('temp/predictor.model')\n",
    "    predictions = bst.predict(dtest)\n",
    "    score = mean_squared_error(test_y, predictions, squared=False)\n",
    "    print(score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4297.474437630029\n"
     ]
    }
   ],
   "source": [
    "predictions = bst.predict(dtest)\n",
    "score = mean_squared_error(test_y, predictions, squared=False)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "dump(scalers, open('temp/scalers.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Scaled Data, used for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's sorty by geo code and date first\n",
    "df_baseline = df.copy()\n",
    "df_baseline = df_baseline.drop(labels=['predicted_new_cases'], axis=1)\n",
    "df_baseline = df_baseline.sort_values(['geo_code', 'date'])\n",
    "\n",
    "filename_prefix = 'prediction-baseline'\n",
    "dir_output_prediction_baseline = 'temp/prediction-baseline'\n",
    "\n",
    "# recreate the directory, deleting any existing content\n",
    "shutil.rmtree(dir_output_prediction_baseline, ignore_errors=True)\n",
    "Path(dir_output_prediction_baseline).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for each geography, write a JSON and CSV file\n",
    "for geo_code in df_baseline['geo_code'].unique():\n",
    "    df_geo = df_baseline.loc[df_baseline['geo_code'] == geo_code].copy()\n",
    "    \n",
    "    # encode and scale\n",
    "    df_geo = encode(df_geo, fit=False)\n",
    "    for scaler_name in scalers:\n",
    "        if scaler_name in df_geo.columns:\n",
    "            df_geo[scaler_name] = scalers[scaler_name].transform(df_geo[[scaler_name]])    \n",
    "    \n",
    "    # write the output\n",
    "    df_geo.to_csv(f\"{dir_output_prediction_baseline}/{filename_prefix}-{geo_code.replace('/', '-')}.csv\", \n",
    "                  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95580 entries, 0 to 138114\n",
      "Data columns (total 178 columns):\n",
      " #    Column                                          Non-Null Count  Dtype         \n",
      "---   ------                                          --------------  -----         \n",
      " 0    predicted_new_cases                             95580 non-null  float64       \n",
      " 1    age_distribution_00_04                          95580 non-null  float64       \n",
      " 2    age_distribution_05_14                          95580 non-null  float64       \n",
      " 3    age_distribution_15_34                          95580 non-null  float64       \n",
      " 4    age_distribution_34_64                          95580 non-null  float64       \n",
      " 5    age_distribution_65_plus                        95580 non-null  float64       \n",
      " 6    c1_school_closing                               95580 non-null  float64       \n",
      " 7    c1_school_closing_ma_21                         95580 non-null  float64       \n",
      " 8    c1_school_closing_ma_3                          95580 non-null  float64       \n",
      " 9    c1_school_closing_ma_7                          95580 non-null  float64       \n",
      " 10   c2_workplace_closing                            95580 non-null  float64       \n",
      " 11   c2_workplace_closing_ma_21                      95580 non-null  float64       \n",
      " 12   c2_workplace_closing_ma_3                       95580 non-null  float64       \n",
      " 13   c2_workplace_closing_ma_7                       95580 non-null  float64       \n",
      " 14   c3_cancel_public_events                         95580 non-null  float64       \n",
      " 15   c3_cancel_public_events_ma_21                   95580 non-null  float64       \n",
      " 16   c3_cancel_public_events_ma_3                    95580 non-null  float64       \n",
      " 17   c3_cancel_public_events_ma_7                    95580 non-null  float64       \n",
      " 18   c4_restrictions_on_gatherings                   95580 non-null  float64       \n",
      " 19   c4_restrictions_on_gatherings_ma_21             95580 non-null  float64       \n",
      " 20   c4_restrictions_on_gatherings_ma_3              95580 non-null  float64       \n",
      " 21   c4_restrictions_on_gatherings_ma_7              95580 non-null  float64       \n",
      " 22   c5_close_public_transport                       95580 non-null  float64       \n",
      " 23   c5_close_public_transport_ma_21                 95580 non-null  float64       \n",
      " 24   c5_close_public_transport_ma_3                  95580 non-null  float64       \n",
      " 25   c5_close_public_transport_ma_7                  95580 non-null  float64       \n",
      " 26   c6_stay_at_home_requirements                    95580 non-null  float64       \n",
      " 27   c6_stay_at_home_requirements_ma_21              95580 non-null  float64       \n",
      " 28   c6_stay_at_home_requirements_ma_3               95580 non-null  float64       \n",
      " 29   c6_stay_at_home_requirements_ma_7               95580 non-null  float64       \n",
      " 30   c7_restrictions_on_internal_movement            95580 non-null  float64       \n",
      " 31   c7_restrictions_on_internal_movement_ma_21      95580 non-null  float64       \n",
      " 32   c7_restrictions_on_internal_movement_ma_3       95580 non-null  float64       \n",
      " 33   c7_restrictions_on_internal_movement_ma_7       95580 non-null  float64       \n",
      " 34   c8_international_travel_controls                95580 non-null  float64       \n",
      " 35   c8_international_travel_controls_ma_21          95580 non-null  float64       \n",
      " 36   c8_international_travel_controls_ma_3           95580 non-null  float64       \n",
      " 37   c8_international_travel_controls_ma_7           95580 non-null  float64       \n",
      " 38   confirmed_cases                                 95580 non-null  float64       \n",
      " 39   confirmed_cases--                               95580 non-null  bool          \n",
      " 40   confirmed_cases_as_percent_of_population        95580 non-null  float64       \n",
      " 41   confirmed_cases_as_percent_of_population_ma_21  95580 non-null  float64       \n",
      " 42   confirmed_cases_as_percent_of_population_ma_3   95580 non-null  float64       \n",
      " 43   confirmed_cases_as_percent_of_population_ma_7   95580 non-null  float64       \n",
      " 44   confirmed_cases_ma_21                           95580 non-null  float64       \n",
      " 45   confirmed_cases_ma_3                            95580 non-null  float64       \n",
      " 46   confirmed_cases_ma_7                            95580 non-null  float64       \n",
      " 47   date                                            95580 non-null  datetime64[ns]\n",
      " 48   gdp_per_capita                                  95580 non-null  float64       \n",
      " 49   gdp_per_capita--                                95580 non-null  bool          \n",
      " 50   h1_public_information_campaigns                 95580 non-null  float64       \n",
      " 51   h1_public_information_campaigns_ma_21           95580 non-null  float64       \n",
      " 52   h1_public_information_campaigns_ma_3            95580 non-null  float64       \n",
      " 53   h1_public_information_campaigns_ma_7            95580 non-null  float64       \n",
      " 54   h2_testing_policy                               95580 non-null  float64       \n",
      " 55   h2_testing_policy_ma_21                         95580 non-null  float64       \n",
      " 56   h2_testing_policy_ma_3                          95580 non-null  float64       \n",
      " 57   h2_testing_policy_ma_7                          95580 non-null  float64       \n",
      " 58   h3_contact_tracing                              95580 non-null  float64       \n",
      " 59   h3_contact_tracing_ma_21                        95580 non-null  float64       \n",
      " 60   h3_contact_tracing_ma_3                         95580 non-null  float64       \n",
      " 61   h3_contact_tracing_ma_7                         95580 non-null  float64       \n",
      " 62   h6_facial_coverings                             95580 non-null  float64       \n",
      " 63   h6_facial_coverings_ma_21                       95580 non-null  float64       \n",
      " 64   h6_facial_coverings_ma_3                        95580 non-null  float64       \n",
      " 65   h6_facial_coverings_ma_7                        95580 non-null  float64       \n",
      " 66   new_cases                                       95580 non-null  float64       \n",
      " 67   new_cases_as_percent_of_population              95580 non-null  float64       \n",
      " 68   new_cases_as_percent_of_population_ma_21        95580 non-null  float64       \n",
      " 69   new_cases_as_percent_of_population_ma_3         95580 non-null  float64       \n",
      " 70   new_cases_as_percent_of_population_ma_7         95580 non-null  float64       \n",
      " 71   new_cases_ma_21                                 95580 non-null  float64       \n",
      " 72   new_cases_ma_3                                  95580 non-null  float64       \n",
      " 73   new_cases_ma_7                                  95580 non-null  float64       \n",
      " 74   npi_sum                                         95580 non-null  float64       \n",
      " 75   obesity_rate                                    95580 non-null  float64       \n",
      " 76   obesity_rate--                                  95580 non-null  bool          \n",
      " 77   pneumonia_deaths_per_100k                       95580 non-null  float64       \n",
      " 78   pneumonia_deaths_per_100k--                     95580 non-null  bool          \n",
      " 79   population                                      95580 non-null  float64       \n",
      " 80   population--                                    95580 non-null  bool          \n",
      " 81   population_density                              95580 non-null  float64       \n",
      " 82   population_density--                            95580 non-null  bool          \n",
      " 83   population_percent_urban                        95580 non-null  float64       \n",
      " 84   population_percent_urban--                      95580 non-null  bool          \n",
      " 85   specific_humidity                               95580 non-null  float64       \n",
      " 86   specific_humidity_ma_21                         95580 non-null  float64       \n",
      " 87   specific_humidity_ma_3                          95580 non-null  float64       \n",
      " 88   specific_humidity_ma_7                          95580 non-null  float64       \n",
      " 89   temperature                                     95580 non-null  float64       \n",
      " 90   temperature_ma_21                               95580 non-null  float64       \n",
      " 91   temperature_ma_3                                95580 non-null  float64       \n",
      " 92   temperature_ma_7                                95580 non-null  float64       \n",
      " 93   working_day                                     95580 non-null  float64       \n",
      " 94   working_day--                                   95580 non-null  bool          \n",
      " 95   working_day_ma_21                               95580 non-null  float64       \n",
      " 96   working_day_ma_3                                95580 non-null  float64       \n",
      " 97   working_day_ma_7                                95580 non-null  float64       \n",
      " 98   working_day_tomorrow                            95580 non-null  float64       \n",
      " 99   working_day_yesterday                           95580 non-null  float64       \n",
      " 100  year                                            95580 non-null  int64         \n",
      " 101  date_day                                        95580 non-null  int64         \n",
      " 102  continent_bin_0                                 95580 non-null  int64         \n",
      " 103  continent_bin_1                                 95580 non-null  int64         \n",
      " 104  continent_bin_2                                 95580 non-null  int64         \n",
      " 105  continent_bin_3                                 95580 non-null  int64         \n",
      " 106  geo_code_bin_0                                  95580 non-null  int64         \n",
      " 107  geo_code_bin_1                                  95580 non-null  int64         \n",
      " 108  geo_code_bin_2                                  95580 non-null  int64         \n",
      " 109  geo_code_bin_3                                  95580 non-null  int64         \n",
      " 110  geo_code_bin_4                                  95580 non-null  int64         \n",
      " 111  geo_code_bin_5                                  95580 non-null  int64         \n",
      " 112  geo_code_bin_6                                  95580 non-null  int64         \n",
      " 113  geo_code_bin_7                                  95580 non-null  int64         \n",
      " 114  geo_code_bin_8                                  95580 non-null  int64         \n",
      " 115  country_code_bin_0                              95580 non-null  int64         \n",
      " 116  country_code_bin_1                              95580 non-null  int64         \n",
      " 117  country_code_bin_2                              95580 non-null  int64         \n",
      " 118  country_code_bin_3                              95580 non-null  int64         \n",
      " 119  country_code_bin_4                              95580 non-null  int64         \n",
      " 120  country_code_bin_5                              95580 non-null  int64         \n",
      " 121  country_code_bin_6                              95580 non-null  int64         \n",
      " 122  country_code_bin_7                              95580 non-null  int64         \n",
      " 123  country_code_bin_8                              95580 non-null  int64         \n",
      " 124  day_of_week_ohe_3.0                             95580 non-null  int64         \n",
      " 125  day_of_week_ohe_4.0                             95580 non-null  int64         \n",
      " 126  day_of_week_ohe_5.0                             95580 non-null  int64         \n",
      " 127  day_of_week_ohe_6.0                             95580 non-null  int64         \n",
      " 128  day_of_week_ohe_7.0                             95580 non-null  int64         \n",
      " 129  day_of_week_ohe_1.0                             95580 non-null  int64         \n",
      " 130  day_of_week_ohe_2.0                             95580 non-null  int64         \n",
      " 131  day_of_week_sin                                 95580 non-null  float64       \n",
      " 132  day_of_week_cos                                 95580 non-null  float64       \n",
      " 133  day_of_month_ohe_1.0                            95580 non-null  int64         \n",
      " 134  day_of_month_ohe_2.0                            95580 non-null  int64         \n",
      " 135  day_of_month_ohe_3.0                            95580 non-null  int64         \n",
      " 136  day_of_month_ohe_4.0                            95580 non-null  int64         \n",
      " 137  day_of_month_ohe_5.0                            95580 non-null  int64         \n",
      " 138  day_of_month_ohe_6.0                            95580 non-null  int64         \n",
      " 139  day_of_month_ohe_7.0                            95580 non-null  int64         \n",
      " 140  day_of_month_ohe_8.0                            95580 non-null  int64         \n",
      " 141  day_of_month_ohe_9.0                            95580 non-null  int64         \n",
      " 142  day_of_month_ohe_10.0                           95580 non-null  int64         \n",
      " 143  day_of_month_ohe_11.0                           95580 non-null  int64         \n",
      " 144  day_of_month_ohe_12.0                           95580 non-null  int64         \n",
      " 145  day_of_month_ohe_13.0                           95580 non-null  int64         \n",
      " 146  day_of_month_ohe_14.0                           95580 non-null  int64         \n",
      " 147  day_of_month_ohe_15.0                           95580 non-null  int64         \n",
      " 148  day_of_month_ohe_16.0                           95580 non-null  int64         \n",
      " 149  day_of_month_ohe_17.0                           95580 non-null  int64         \n",
      " 150  day_of_month_ohe_18.0                           95580 non-null  int64         \n",
      " 151  day_of_month_ohe_19.0                           95580 non-null  int64         \n",
      " 152  day_of_month_ohe_20.0                           95580 non-null  int64         \n",
      " 153  day_of_month_ohe_21.0                           95580 non-null  int64         \n",
      " 154  day_of_month_ohe_22.0                           95580 non-null  int64         \n",
      " 155  day_of_month_ohe_23.0                           95580 non-null  int64         \n",
      " 156  day_of_month_ohe_24.0                           95580 non-null  int64         \n",
      " 157  day_of_month_ohe_25.0                           95580 non-null  int64         \n",
      " 158  day_of_month_ohe_26.0                           95580 non-null  int64         \n",
      " 159  day_of_month_ohe_27.0                           95580 non-null  int64         \n",
      " 160  day_of_month_ohe_28.0                           95580 non-null  int64         \n",
      " 161  day_of_month_ohe_29.0                           95580 non-null  int64         \n",
      " 162  day_of_month_ohe_30.0                           95580 non-null  int64         \n",
      " 163  day_of_month_ohe_31.0                           95580 non-null  int64         \n",
      " 164  day_of_month_sin                                95580 non-null  float64       \n",
      " 165  day_of_month_cos                                95580 non-null  float64       \n",
      " 166  day_of_year_bin_0                               95580 non-null  int64         \n",
      " 167  day_of_year_bin_1                               95580 non-null  int64         \n",
      " 168  day_of_year_bin_2                               95580 non-null  int64         \n",
      " 169  day_of_year_bin_3                               95580 non-null  int64         \n",
      " 170  day_of_year_bin_4                               95580 non-null  int64         \n",
      " 171  day_of_year_bin_5                               95580 non-null  int64         \n",
      " 172  day_of_year_bin_6                               95580 non-null  int64         \n",
      " 173  day_of_year_bin_7                               95580 non-null  int64         \n",
      " 174  day_of_year_bin_8                               95580 non-null  int64         \n",
      " 175  day_of_year_bin_9                               95580 non-null  int64         \n",
      " 176  day_of_year_sin                                 95580 non-null  float64       \n",
      " 177  day_of_year_cos                                 95580 non-null  float64       \n",
      "dtypes: bool(8), datetime64[ns](1), float64(97), int64(72)\n",
      "memory usage: 127.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
