{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search Update Notebook\n",
    "\n",
    "This notebook is used to update the elastic search index with the latest datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sagemaker import get_execution_role, session\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from elasticsearch import helpers\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from logging import INFO, basicConfig, info\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "basicConfig(level=INFO, format='%(asctime)s\\t%(levelname)s\\t%(filename)s\\t%(message)s')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # ignore FutureWarning from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_info_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temp/01-data.csv', keep_default_na=False, na_values='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ma(field, window_size):\n",
    "    df[f\"{field}_ma_{window_size}\"] = df.groupby('geo_code')[field].rolling(window_size, center=False).mean().fillna(0).reset_index(0, drop=True)\n",
    "\n",
    "def add_working_day_tomorrow(grouped):\n",
    "    grouped['working_day' + '_tomorrow'] = grouped['working_day'].copy().shift(-1).bfill().ffill()\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def add_working_day_yesterday(grouped):\n",
    "    grouped['working_day' + '_yesterday'] = grouped['working_day'].copy().shift(1).bfill().ffill()\n",
    "    return grouped\n",
    "\n",
    "def transform_column_order(df):\n",
    "    df = df.reindex(sorted(df.columns), axis=1)  # Sort columns by name\n",
    "    df_label = df['predicted_new_cases']\n",
    "    df = df.drop(labels=['predicted_new_cases'], axis=1)\n",
    "    df.insert(0, 'predicted_new_cases', df_label)\n",
    "    return df\n",
    "\n",
    "# Compute number of new cases and deaths each day\n",
    "# Replace negative values (which do not make sense for these columns) with 0\n",
    "df['new_cases'] = df.groupby('geo_code').confirmed_cases.diff().fillna(0)\n",
    "df['new_cases'] = df['new_cases'].clip(lower=0)\n",
    "\n",
    "# add predicted new cases\n",
    "df['predicted_new_cases'] = df.groupby('geo_code').new_cases.shift(-1).fillna(0)\n",
    "df['predicted_new_cases'] = df['predicted_new_cases'].clip(lower=0)\n",
    "\n",
    "# add confirmed cases as percent of population\n",
    "df['new_cases_as_percent_of_population'] = df['new_cases'] / df['population']\n",
    "df['confirmed_cases_as_percent_of_population'] = df['confirmed_cases'] / df['population']\n",
    "\n",
    "# Add moving averages\n",
    "for window_size in [3, 7, 21]:\n",
    "    compute_ma('new_cases', window_size)\n",
    "    compute_ma('confirmed_cases', window_size)\n",
    "    compute_ma('specific_humidity', window_size)    \n",
    "    compute_ma('temperature', window_size)        \n",
    "    compute_ma('c1_school_closing', window_size)        \n",
    "    compute_ma('c2_workplace_closing', window_size)        \n",
    "    compute_ma('c3_cancel_public_events', window_size)        \n",
    "    compute_ma('c4_restrictions_on_gatherings', window_size)   \n",
    "    compute_ma('c5_close_public_transport', window_size)        \n",
    "    compute_ma('c6_stay_at_home_requirements', window_size)        \n",
    "    compute_ma('c7_restrictions_on_internal_movement', window_size)        \n",
    "    compute_ma('c8_international_travel_controls', window_size)   \n",
    "    compute_ma('h1_public_information_campaigns', window_size)        \n",
    "    compute_ma('h2_testing_policy', window_size)        \n",
    "    compute_ma('h3_contact_tracing', window_size)        \n",
    "    compute_ma('h6_facial_coverings', window_size)     \n",
    "    compute_ma('working_day', window_size)        \n",
    "    compute_ma('new_cases_as_percent_of_population', window_size)     \n",
    "    compute_ma('confirmed_cases_as_percent_of_population', window_size)     \n",
    "\n",
    "# Add working day information for tomorrow, and today\n",
    "df = df.groupby('geo_code').apply(lambda group: add_working_day_tomorrow(group)).reset_index(drop=True)\n",
    "df = df.groupby('geo_code').apply(lambda group: add_working_day_yesterday(group)).reset_index(drop=True)\n",
    "df['npi_sum'] = df['c1_school_closing'] + df['c2_workplace_closing'] + \\\n",
    "                df['c3_cancel_public_events'] + df['c4_restrictions_on_gatherings'] + \\\n",
    "                df['c5_close_public_transport'] + df['c6_stay_at_home_requirements'] + \\\n",
    "                df['c7_restrictions_on_internal_movement'] + df['c8_international_travel_controls'] + \\\n",
    "                df['h1_public_information_campaigns'] + df['h2_testing_policy'] + \\\n",
    "                df['h3_contact_tracing'] + df['h6_facial_coverings']\n",
    "\n",
    "# Drop unused columns\n",
    "df = transform_column_order(df)\n",
    "df = df.sort_values(['geo_code', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work within the specified range\n",
    "df_ml = df.loc[df['date'] < pd.to_datetime(prediction_start_date)]\n",
    "df_ml = df_ml.drop(labels=['country_name',\n",
    "                        'country_code3',\n",
    "                        'country_code_numeric',\n",
    "                        'confirmed_deaths',\n",
    "                        'region_name',\n",
    "                        'month',\n",
    "                        'quarter',\n",
    "                        'week'], axis=1)\n",
    "\n",
    "def encode(df_x):\n",
    "    # convert the date to an integer value\n",
    "    df_x['date_day'] = df_x['date'].apply(lambda x: x.day)\n",
    "\n",
    "    # encode the geo data\n",
    "    df_x = encoders.BinaryEncoder('continent').fit_transform(df_x).drop(labels=['continent'], axis=1)\n",
    "    df_x = encoders.BinaryEncoder('geo_code').fit_transform(df_x).drop(labels=['geo_code'], axis=1)\n",
    "    df_x = encoders.BinaryEncoder('country_code').fit_transform(df_x).drop(labels=['country_code'], axis=1)\n",
    "\n",
    "    # dy of week\n",
    "    df_x = encoders.OneHotEncoder('day_of_week').fit_transform(df_x)\n",
    "    df_x['day_of_week'] = df_x['day_of_week'] / 7.0\n",
    "    df_x = encoders.CyclicalEncoder('day_of_week').fit_transform(df_x).drop(labels=['day_of_week'], axis=1)\n",
    "\n",
    "    # day of month\n",
    "    df_x['day_of_month'] = df_x['day_of_month'] / 31.0 # keep it simple\n",
    "    df_x = encoders.BinaryEncoder('day_of_month').fit_transform(df_x)\n",
    "    df_x = encoders.CyclicalEncoder('day_of_month').fit_transform(df_x).drop(labels=['day_of_month'], axis=1)\n",
    "\n",
    "    # day of year\n",
    "    df_x['day_of_year'] = df_x['day_of_year'] / 366.0 # keep it simple\n",
    "    df_x = encoders.BinaryEncoder('day_of_year').fit_transform(df_x)\n",
    "    df_x = encoders.CyclicalEncoder('day_of_year').fit_transform(df_x).drop(labels=['day_of_year'], axis=1)\n",
    "    return df_x\n",
    "\n",
    "df_ml = encode(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train, val, test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_for_validation = 0\n",
    "days_for_test = 14\n",
    "\n",
    "def split(df: pd.DataFrame, \n",
    "          days_for_validation: int, \n",
    "          days_for_test: int) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # First, sort the data by date\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Determine the maximum date\n",
    "    date_start_test = df[DATE].max() - pd.to_timedelta(days_for_test - 1, unit='d')\n",
    "    date_start_validation = date_start_test - pd.to_timedelta(days_for_validation, unit='d')\n",
    "\n",
    "    df_train = df[df['date'] < date_start_validation]\n",
    "    df_validation = df[(df['date'] >= date_start_validation) & (df['date'] < date_start_test)]\n",
    "    df_test = df[df['date'] >= date_start_test]\n",
    "\n",
    "    # Debug the outpoint\n",
    "    print(f\"Training Range:   {df_train['date'].min().date()} - {df_train['date'].max().date()}\")\n",
    "    print(f\"Validation Range: {df_validation['date'].min().date()} - {df_validation['date'].max().date()}\")\n",
    "    print(f\"Test Range:       {df_test['date'].min().date()} - {df_test['date'].max().date()}\")\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(df.index) != len(df_train.index) + len(df_validation.index) + len(df_test.index):\n",
    "        raise Exception('entries do not add up')\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "df_train_prescaled, df_validation_prescaled, df_test_prescaled = split(df_ml, days_for_validation, days_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_train = df_train_prescaled.copy()\n",
    "df_validation = df_validation_prescaled.copy()\n",
    "df_test = df_test_prescaled.copy()\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "for feature_name in df_ml.columns.values:\n",
    "    if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "        continue\n",
    "    scalers[feature_name] = RobustScaler()\n",
    "    df_train[feature_name] = scalers[feature_name].fit_transform(df_train_prescaled[[feature_name]])\n",
    "        \n",
    "if len(df_validation_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue\n",
    "        df_validation[feature_name] = scalers[feature_name].transform(df_validation_prescaled[[feature_name]])\n",
    "\n",
    "if len(df_test_prescaled) > 0:        \n",
    "    for feature_name in df_ml.columns.values:\n",
    "        if feature_name == 'date' or feature_name == 'predicted_new_cases':\n",
    "            continue        \n",
    "        df_test[feature_name] = scalers[feature_name].transform(df_test_prescaled[[feature_name]])\n",
    "\n",
    "df_train = df_train.drop(labels=['date'], axis=1)\n",
    "df_validation = df_validation.drop(labels=['date'], axis=1)\n",
    "df_test = df_test.drop(labels=['date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO Baby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is a manual step to determine the best parameters...\n",
    "\n",
    "# note that HPO does not have a header column\n",
    "\"\"\"\n",
    "df_train.to_csv(f\"hpo_train.csv\", index=False, header=False)\n",
    "df_validation.to_csv(f\"hpo_validation.csv\", index=False, header=False)\n",
    "df_test.to_csv(f\"hpo_test.csv\", index=False, header=False)\n",
    "\n",
    "# push to s3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('hpo', 'train')).upload_file('hpo_train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('hpo', 'validation')).upload_file('hpo_validation.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('hpo', 'test')).upload_file('hpo_test.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:7030.52534+146.13767\ttest-rmse:7017.84634+1264.82343\n",
      "[1]\ttrain-rmse:6014.82485+152.46684\ttest-rmse:6051.61816+1363.17175\n",
      "[2]\ttrain-rmse:5153.38862+151.06150\ttest-rmse:5271.16826+1461.30391\n",
      "[3]\ttrain-rmse:4452.34253+162.47070\ttest-rmse:4639.33069+1552.80275\n",
      "[4]\ttrain-rmse:3857.96174+167.17000\ttest-rmse:4126.22151+1645.06974\n",
      "[5]\ttrain-rmse:3365.56177+160.58723\ttest-rmse:3722.10637+1718.97338\n",
      "[6]\ttrain-rmse:2938.35625+157.62463\ttest-rmse:3396.18481+1778.71113\n",
      "[7]\ttrain-rmse:2574.99138+155.65825\ttest-rmse:3147.93709+1830.01902\n",
      "[8]\ttrain-rmse:2282.22495+156.46754\ttest-rmse:2971.96692+1868.51628\n",
      "[9]\ttrain-rmse:2041.65498+163.99757\ttest-rmse:2819.78458+1900.49468\n",
      "[10]\ttrain-rmse:1818.10314+152.91726\ttest-rmse:2690.87959+1929.98852\n",
      "[11]\ttrain-rmse:1618.93767+143.72061\ttest-rmse:2602.85756+1949.31768\n",
      "[12]\ttrain-rmse:1446.69102+137.13164\ttest-rmse:2537.62733+1963.47213\n",
      "[13]\ttrain-rmse:1308.88207+128.32482\ttest-rmse:2488.94933+1975.21751\n",
      "[14]\ttrain-rmse:1186.30573+126.88331\ttest-rmse:2448.83387+1985.25227\n",
      "[15]\ttrain-rmse:1074.61232+119.69671\ttest-rmse:2421.41803+1992.09705\n",
      "[16]\ttrain-rmse:986.08441+113.19193\ttest-rmse:2400.12272+1997.49452\n",
      "[17]\ttrain-rmse:899.69397+114.53371\ttest-rmse:2404.22275+1994.56204\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "train_x, train_y, validation_x, validation_y, test_x, test_y = (\n",
    "    df_train.iloc[:, 1:], df_train.iloc[:, :1],\n",
    "    df_validation.iloc[:, 1:], df_validation.iloc[:, :1],\n",
    "    df_test.iloc[:, 1:], df_test.iloc[:, :1])\n",
    "\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=train_x,label=train_y)\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'alpha': 0.734,\n",
    "    #\"booster\": \"gbtree\",\n",
    "    'colsample_bytree': 0.9247351903173018,\n",
    "    \"colsample_bylevel\": 0.11338785332863967,\n",
    "    \"eta\": 0.2857594277665818,\n",
    "    \"gamma\": 0.7840414267431137,\n",
    "    #'learning_rate': 0.1,\n",
    "    \"max_delta_step\": 0,\n",
    "    'max_depth': 43, \n",
    "    \"max_leaves\": 0,\n",
    "    \"min_child_weight\": 0.015174585370894897,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"subsample\": 0.5982549859025215\n",
    "}\n",
    "\n",
    "\n",
    "params_baseline = {\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective':'reg:squarederror'\n",
    "}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=10,\n",
    "                    num_boost_round=20,\n",
    "                    early_stopping_rounds=10,\n",
    "                    metrics=\"rmse\", \n",
    "                    as_pandas=True,\n",
    "                    verbose_eval=True,\n",
    "                    seed=123)\n",
    "print(cv_results.tail(100))\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0         14.448232        0.457032       14.128955       3.542234\n",
      "1         13.281850        0.453128       13.022488       3.631210\n",
      "2         12.247323        0.451868       12.034971       3.731681\n",
      "3         11.357578        0.527235       11.216321       3.785879\n",
      "4         10.516548        0.530564       10.433459       3.872672\n",
      "5          9.828135        0.591498        9.789183       3.954284\n",
      "6          9.142688        0.583163        9.192409       4.022693\n",
      "7          8.485017        0.562325        8.619303       4.099982\n",
      "8          7.929716        0.564755        8.147279       4.160788\n",
      "9          7.432303        0.567991        7.728239       4.210274\n",
      "10         6.981730        0.556943        7.346896       4.272597\n",
      "11         6.590199        0.541217        7.070919       4.311739\n",
      "12         6.198319        0.515502        6.778163       4.365013\n",
      "13         5.871631        0.526149        6.519623       4.408850\n",
      "14         5.550954        0.511343        6.322921       4.434454\n",
      "15         5.253249        0.497273        6.148002       4.466876\n",
      "16         4.978561        0.478242        6.014939       4.489387\n",
      "17         4.737716        0.463097        5.878008       4.515185\n",
      "18         4.545918        0.443568        5.752903       4.543236\n",
      "19         4.355068        0.422794        5.648374       4.563398\n",
      "20         4.164886        0.398850        5.597344       4.568853\n",
      "21         3.997969        0.384005        5.570953       4.565417\n",
      "22         3.844839        0.381185        5.511651       4.577016\n",
      "23         3.716739        0.382273        5.450828       4.588943\n",
      "24         3.596023        0.362955        5.410507       4.598999\n",
      "25         3.483682        0.345356        5.360471       4.609237\n",
      "26         3.374824        0.324267        5.323879       4.619020\n",
      "27         3.262502        0.306702        5.302558       4.618914\n",
      "28         3.167116        0.292636        5.280013       4.620688\n",
      "29         3.085901        0.283167        5.252543       4.625451\n",
      "30         3.013424        0.269086        5.238926       4.630011\n",
      "31         2.945509        0.258509        5.226781       4.632471\n",
      "32         2.874866        0.254227        5.211782       4.634405\n",
      "33         2.807517        0.243511        5.194733       4.638130\n",
      "34         2.754406        0.237938        5.188900       4.640938\n",
      "35         2.692423        0.223158        5.195022       4.642009\n",
      "36         2.642345        0.210679        5.203412       4.645792\n",
      "37         2.594001        0.198309        5.199613       4.644453\n",
      "38         2.546453        0.189140        5.188945       4.643300\n",
      "39         2.504218        0.180141        5.180830       4.644805\n",
      "40         2.468708        0.171682        5.173982       4.645214\n",
      "41         2.431862        0.165679        5.175155       4.650189\n",
      "42         2.399251        0.159083        5.187775       4.650786\n",
      "43         2.370319        0.153248        5.182965       4.651414\n",
      "44         2.350453        0.151331        5.180339       4.652546\n",
      "45         2.321485        0.150179        5.186307       4.654718\n",
      "46         2.306253        0.149085        5.180598       4.657360\n",
      "47         2.283379        0.145438        5.175846       4.659467\n",
      "48         2.258853        0.138594        5.168695       4.660821\n",
      "49         2.239662        0.133720        5.162439       4.662839\n"
     ]
    }
   ],
   "source": [
    "print(cv_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the model, scalers, and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
